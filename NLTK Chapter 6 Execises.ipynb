{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import names\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Using any of the three classifiers described in this chapter, and any features you can think of, build the best name gender classifier you can. Begin by splitting the Names Corpus into three subsets: 500 words for the test set, 500 words for the dev-test set, and the remaining 6900 words for the training set. Then, starting with the example name gender classifier, make incremental improvements. Use the dev-test set to check your progress. Once you are satisfied with your classifier, check its final performance on the test set. How does the performance on the test set compare to the performance on the dev-test set? Is this what you'd expect?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_feature(name):\n",
    "    return {'suffix[1]':name[-1],\n",
    "           'suffix[2]': name[-2],\n",
    "           'startswith': name[0]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_names = [(name, 'male') for name in names.words('male.txt')] + [(name, 'female') for name in names.words('female.txt')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7944"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labeled_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random.shuffle(labeled_names)\n",
    "feature_set = [(get_feature(name), label) for name, label in labeled_names]\n",
    "train_set, dev_set, test_set = feature_set[:6000], feature_set[6000:7000], feature_set[7000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.789\n",
      "0.7796610169491526\n"
     ]
    }
   ],
   "source": [
    "csf1 = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print(nltk.classify.accuracy(csf1, dev_set))\n",
    "print(nltk.classify.accuracy(csf1, test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.791\n",
      "0.7796610169491526\n"
     ]
    }
   ],
   "source": [
    "csf2 = nltk.DecisionTreeClassifier.train(train_set)\n",
    "print(nltk.classify.accuracy(csf2, dev_set))\n",
    "print(nltk.classify.accuracy(csf2, test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.374\n",
      "             2          -0.45425        0.757\n",
      "             3          -0.39822        0.784\n",
      "             4          -0.37251        0.788\n",
      "             5          -0.35851        0.793\n",
      "             6          -0.35000        0.793\n",
      "             7          -0.34442        0.796\n",
      "             8          -0.34058        0.796\n",
      "             9          -0.33781        0.796\n",
      "            10          -0.33577        0.796\n",
      "            11          -0.33421        0.800\n",
      "            12          -0.33300        0.799\n",
      "            13          -0.33204        0.798\n",
      "            14          -0.33127        0.797\n",
      "            15          -0.33064        0.798\n",
      "            16          -0.33012        0.799\n",
      "            17          -0.32969        0.799\n",
      "            18          -0.32933        0.800\n",
      "            19          -0.32902        0.800\n",
      "            20          -0.32876        0.801\n",
      "            21          -0.32854        0.801\n",
      "            22          -0.32834        0.801\n",
      "            23          -0.32817        0.801\n",
      "            24          -0.32802        0.799\n",
      "            25          -0.32790        0.799\n",
      "            26          -0.32778        0.799\n",
      "            27          -0.32768        0.800\n",
      "            28          -0.32759        0.799\n",
      "            29          -0.32751        0.799\n",
      "            30          -0.32744        0.799\n",
      "            31          -0.32738        0.799\n",
      "            32          -0.32732        0.799\n",
      "            33          -0.32727        0.799\n",
      "            34          -0.32723        0.799\n",
      "            35          -0.32718        0.799\n",
      "            36          -0.32715        0.799\n",
      "            37          -0.32711        0.799\n",
      "            38          -0.32708        0.799\n",
      "            39          -0.32705        0.799\n",
      "            40          -0.32703        0.799\n",
      "            41          -0.32700        0.799\n",
      "            42          -0.32698        0.799\n",
      "            43          -0.32696        0.799\n",
      "            44          -0.32694        0.799\n",
      "            45          -0.32693        0.799\n",
      "            46          -0.32691        0.799\n",
      "            47          -0.32690        0.799\n",
      "            48          -0.32688        0.799\n",
      "            49          -0.32687        0.799\n",
      "            50          -0.32686        0.799\n",
      "            51          -0.32685        0.799\n",
      "            52          -0.32684        0.799\n",
      "            53          -0.32683        0.799\n",
      "            54          -0.32682        0.799\n",
      "            55          -0.32681        0.799\n",
      "            56          -0.32680        0.799\n",
      "            57          -0.32680        0.799\n",
      "            58          -0.32679        0.799\n",
      "            59          -0.32678        0.799\n",
      "            60          -0.32677        0.799\n",
      "            61          -0.32677        0.799\n",
      "            62          -0.32676        0.799\n",
      "            63          -0.32676        0.799\n",
      "            64          -0.32675        0.799\n",
      "            65          -0.32675        0.799\n",
      "            66          -0.32674        0.799\n",
      "            67          -0.32674        0.799\n",
      "            68          -0.32673        0.799\n",
      "            69          -0.32673        0.799\n",
      "            70          -0.32673        0.799\n",
      "            71          -0.32672        0.799\n",
      "            72          -0.32672        0.799\n",
      "            73          -0.32671        0.799\n",
      "            74          -0.32671        0.799\n",
      "            75          -0.32671        0.799\n",
      "            76          -0.32670        0.799\n",
      "            77          -0.32670        0.799\n",
      "            78          -0.32670        0.799\n",
      "            79          -0.32669        0.799\n",
      "            80          -0.32669        0.799\n",
      "            81          -0.32669        0.799\n",
      "            82          -0.32669        0.799\n",
      "            83          -0.32668        0.799\n",
      "            84          -0.32668        0.799\n",
      "            85          -0.32668        0.799\n",
      "            86          -0.32668        0.799\n",
      "            87          -0.32667        0.799\n",
      "            88          -0.32667        0.799\n",
      "            89          -0.32667        0.799\n",
      "            90          -0.32667        0.799\n",
      "            91          -0.32666        0.799\n",
      "            92          -0.32666        0.799\n",
      "            93          -0.32666        0.799\n",
      "            94          -0.32666        0.799\n",
      "            95          -0.32666        0.799\n",
      "            96          -0.32665        0.799\n",
      "            97          -0.32665        0.799\n",
      "            98          -0.32665        0.799\n",
      "            99          -0.32665        0.799\n",
      "         Final          -0.32665        0.799\n",
      "0.797\n",
      "0.8050847457627118\n"
     ]
    }
   ],
   "source": [
    "csf2 = nltk.MaxentClassifier.train(train_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.797\n",
      "0.8050847457627118\n"
     ]
    }
   ],
   "source": [
    "print(nltk.classify.accuracy(csf2, dev_set))\n",
    "print(nltk.classify.accuracy(csf2, test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_feature(name):\n",
    "    features = {}\n",
    "    features[\"first_letter\"] = name[0].lower()\n",
    "    features[\"last_letter\"] = name[-1].lower()\n",
    "    for letter in 'abcdefghijklmnopqrstuvwxyz':\n",
    "        features[\"count({})\".format(letter)] = name.lower().count(letter)\n",
    "        features[\"has({})\".format(letter)] = (letter in name.lower())\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random.shuffle(labeled_names)\n",
    "feature_set = [(get_feature(name), label) for name, label in labeled_names]\n",
    "train_set, dev_set, test_set = feature_set[:6000], feature_set[6000:7000], feature_set[7000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.744\n",
      "0.7701271186440678\n"
     ]
    }
   ],
   "source": [
    "csf1 = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print(nltk.classify.accuracy(csf1, dev_set))\n",
    "print(nltk.classify.accuracy(csf1, test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.787\n",
      "0.798728813559322\n"
     ]
    }
   ],
   "source": [
    "csf2 = nltk.DecisionTreeClassifier.train(train_set)\n",
    "print(nltk.classify.accuracy(csf2, dev_set))\n",
    "print(nltk.classify.accuracy(csf2, test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.374\n",
      "             2          -0.61409        0.626\n",
      "             3          -0.59938        0.626\n",
      "             4          -0.58553        0.627\n",
      "             5          -0.57253        0.635\n",
      "             6          -0.56035        0.652\n",
      "             7          -0.54893        0.674\n",
      "             8          -0.53824        0.692\n",
      "             9          -0.52822        0.707\n",
      "            10          -0.51883        0.719\n",
      "            11          -0.51003        0.730\n",
      "            12          -0.50177        0.739\n",
      "            13          -0.49402        0.745\n",
      "            14          -0.48673        0.753\n",
      "            15          -0.47987        0.760\n",
      "            16          -0.47340        0.766\n",
      "            17          -0.46731        0.770\n",
      "            18          -0.46156        0.771\n",
      "            19          -0.45612        0.774\n",
      "            20          -0.45097        0.775\n",
      "            21          -0.44610        0.778\n",
      "            22          -0.44148        0.780\n",
      "            23          -0.43710        0.782\n",
      "            24          -0.43293        0.782\n",
      "            25          -0.42897        0.783\n",
      "            26          -0.42520        0.785\n",
      "            27          -0.42161        0.787\n",
      "            28          -0.41818        0.788\n",
      "            29          -0.41492        0.790\n",
      "            30          -0.41180        0.790\n",
      "            31          -0.40881        0.791\n",
      "            32          -0.40596        0.792\n",
      "            33          -0.40323        0.793\n",
      "            34          -0.40061        0.794\n",
      "            35          -0.39811        0.794\n",
      "            36          -0.39570        0.794\n",
      "            37          -0.39339        0.794\n",
      "            38          -0.39117        0.794\n",
      "            39          -0.38904        0.794\n",
      "            40          -0.38700        0.794\n",
      "            41          -0.38502        0.795\n",
      "            42          -0.38313        0.796\n",
      "            43          -0.38130        0.796\n",
      "            44          -0.37954        0.796\n",
      "            45          -0.37784        0.797\n",
      "            46          -0.37620        0.797\n",
      "            47          -0.37462        0.798\n",
      "            48          -0.37309        0.798\n",
      "            49          -0.37162        0.798\n",
      "            50          -0.37020        0.798\n",
      "            51          -0.36882        0.798\n",
      "            52          -0.36749        0.798\n",
      "            53          -0.36620        0.799\n",
      "            54          -0.36495        0.798\n",
      "            55          -0.36374        0.798\n",
      "            56          -0.36257        0.798\n",
      "            57          -0.36144        0.798\n",
      "            58          -0.36034        0.799\n",
      "            59          -0.35927        0.799\n",
      "            60          -0.35823        0.800\n",
      "            61          -0.35723        0.800\n",
      "            62          -0.35626        0.800\n",
      "            63          -0.35531        0.800\n",
      "            64          -0.35439        0.801\n",
      "            65          -0.35350        0.801\n",
      "            66          -0.35263        0.801\n",
      "            67          -0.35178        0.802\n",
      "            68          -0.35096        0.802\n",
      "            69          -0.35016        0.802\n",
      "            70          -0.34939        0.802\n",
      "            71          -0.34863        0.802\n",
      "            72          -0.34789        0.802\n",
      "            73          -0.34718        0.802\n",
      "            74          -0.34648        0.803\n",
      "            75          -0.34580        0.803\n",
      "            76          -0.34513        0.803\n",
      "            77          -0.34448        0.803\n",
      "            78          -0.34385        0.802\n",
      "            79          -0.34324        0.803\n",
      "            80          -0.34264        0.803\n",
      "            81          -0.34205        0.803\n",
      "            82          -0.34148        0.803\n",
      "            83          -0.34092        0.803\n",
      "            84          -0.34038        0.803\n",
      "            85          -0.33985        0.803\n",
      "            86          -0.33933        0.803\n",
      "            87          -0.33882        0.803\n",
      "            88          -0.33832        0.803\n",
      "            89          -0.33784        0.803\n",
      "            90          -0.33736        0.803\n",
      "            91          -0.33690        0.804\n",
      "            92          -0.33645        0.804\n",
      "            93          -0.33600        0.804\n",
      "            94          -0.33557        0.804\n",
      "            95          -0.33514        0.804\n",
      "            96          -0.33473        0.804\n",
      "            97          -0.33432        0.804\n",
      "            98          -0.33392        0.805\n",
      "            99          -0.33353        0.804\n",
      "         Final          -0.33315        0.805\n"
     ]
    }
   ],
   "source": [
    "csf2 = nltk.MaxentClassifier.train(train_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.787\n",
      "0.8008474576271186\n"
     ]
    }
   ],
   "source": [
    "print(nltk.classify.accuracy(csf2, dev_set))\n",
    "print(nltk.classify.accuracy(csf2, test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
