{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing Sentence Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk, pprint\n",
    "from nltk.corpus import treebank, ppattach\n",
    "from collections import defaultdict\n",
    "from nltk import Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some Grammatical Dilemmas\n",
    "##### Linguistic Data and Unlimited Possibilities\n",
    "\n",
    "We can concoct new sentences, one which has never been used in a language, which can understood by all speakers of the language. Sentences have a structure which can allow them to be extended, indefinitely. Sentences can be embedded inside larger sentences. (The last two sentences are actually indentical).\n",
    "\n",
    "These structures are represented by a grammar. The purpose of a grammar is to give an explicit description of language. Is grammar largely finite set of oserved utterances and written texts? \n",
    "\n",
    "We're goonna consider 'language' to be an enormous collection of all grammatical sentences and a grammer is a formal notation that can be used for 'generating' the members of this set. Grammars use recursive productions of the form S -> S and S\n",
    "\n",
    "##### Ubiquitous Ambiguity\n",
    "\n",
    "> While hunting in Africa, **I shot an elephant in my pajamas**. How he got into my pajamas, I don't know."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gr_grammar = nltk.CFG.fromstring(\"\"\"\n",
    "S -> NP VP\n",
    "PP -> P NP\n",
    "NP -> Det N | Det N PP | 'I'\n",
    "VP -> V NP | VP PP\n",
    "Det -> 'an' | 'my'\n",
    "N -> 'elephant' | 'pajamas'\n",
    "V -> 'shot'\n",
    "P -> 'in'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP I)\n",
      "  (VP\n",
      "    (VP (V shot) (NP (Det an) (N elephant)))\n",
      "    (PP (P in) (NP (Det my) (N pajamas)))))\n",
      "(S\n",
      "  (NP I)\n",
      "  (VP\n",
      "    (V shot)\n",
      "    (NP (Det an) (N elephant) (PP (P in) (NP (Det my) (N pajamas))))))\n"
     ]
    }
   ],
   "source": [
    "sent = ['I', 'shot', 'an', 'elephant', 'in', 'my', 'pajamas']\n",
    "\n",
    "parser = nltk.ChartParser(gr_grammar)\n",
    "for tree in parser.parse(sent):\n",
    "    print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gr_grammar = nltk.CFG.fromstring(\"\"\"\n",
    "S -> VP NP\n",
    "PP -> P NP\n",
    "NP -> Det N | Det N PP | 'I' | N | P\n",
    "VP -> V NP | VP PP | V\n",
    "Det -> 'an' | 'my' |'you'\n",
    "N -> 'elephant' | 'pajamas' | 'Youtube' |'shape'\n",
    "V -> 'shot' | 'close' | 'open' | 'play'\n",
    "P -> 'in' | 'on' | 'of'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = ['play', 'Brisingr', 'on', 'Youtube']\n",
    "parser = nltk.ChartParser(gr_grammar)\n",
    "# s = parser.parse(sent)\n",
    "for tree in parser.parse(sent):\n",
    "    print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(nltk.ne_chunk(nltk.pos_tag(sent)).draw())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preposition**: a word governing, and usually preceding, a noun or pronoun and expressing a relation to another word or element in the clause, as in ‘the man on the platform’, ‘she arrived after dinner’, ‘what did you do it for ?’.\n",
    "\n",
    "There's no ambiguity in the meaning of any word. For example, 'shot' only refers to the act of using a gun but not taking a picture from the camera.\n",
    "\n",
    "Notice there are two grammars above . One which takes the first rule of VP and other uses the second.\n",
    "\n",
    "### What's the Use of Syntax?\n",
    "##### Beyond the n-grams\n",
    "\n",
    "We can use frequency information in bigrams to generate text that seems perfectly acceptable for small sequences of words but rapidly degenerated into nonsense. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "living creature that he said , and the land of the land of the land "
     ]
    }
   ],
   "source": [
    "text = nltk.corpus.genesis.words('english-kjv.txt')\n",
    "bigrams = nltk.bigrams(text)\n",
    "cfd = nltk.ConditionalFreqDist(bigrams)\n",
    "\n",
    "def generate_words(cfd, word, num =15):\n",
    "    for i in range(num):\n",
    "        print(word, end = ' ')\n",
    "        word = cfd[word].max()\n",
    "\n",
    "generate_words(cfd, 'living')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "love , and the land of the land of the land of the land of "
     ]
    }
   ],
   "source": [
    "generate_words(cfd, 'love')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examples : \n",
    "1. He roared with me the pail slip down his back.\n",
    "2. The worst part and tclumsy looking for whoever heard light\n",
    "\n",
    "You intuitively understand that these sequences are 'word-salad' ( in other words, shit) but yu find it hard to pind down what's wrong with them. \n",
    "So one benefit of studying grammar is that it provides a conceptual framework and vocabulary for spelling out these intuitions. \n",
    "\n",
    "*the worst part and clumsy looking* looks like a **coordinate structure**, where two phrases are joinf by a coordination conjuction such as *and, but* or *or*\n",
    "\n",
    "Coordinate Structure:\n",
    "\n",
    "if **v1** and **v2** are both phrases of grmaatical category X, then **v1 and v2** is also a phrase of category X\n",
    "\n",
    ">a.\t\tThe book's ending was (NP the worst part and the best part) for me.\n",
    "\n",
    ">b.\t\tOn land they are (AP slow and clumsy looking).\n",
    "\n",
    "In the first, two NPs (noun phrases) have been conjoined to make an NP, while in the second, two APs (adjective phrases) have been conjoined to make an AP.\n",
    " \n",
    "> What we can't do is conjoin an NP and an AP, which is why *the worst parst* and clumsy looking* is ungrammatical.\n",
    "\n",
    "Before we can formalize these ideas, we need to understand the concept of **constituent structure**.\n",
    "\n",
    "Constituent structure is based on the observation that words combine with other words to form units. The evidence that a sequence of words forms such a unit is given subsitutability - that is , a sequence of words in a well-formaed sentence can be replaced by a shorted sequence without rendering the sentence ill-formed or changing its meaning. \n",
    "\n",
    "> The little bear saw the fine fat trout in the brook.\n",
    "\n",
    "The fact that we can substitute He for *The little bear* indicates that the latter sequence is a unit. By contrast, we cannot replace *little bear saw* in the same way.\n",
    "\n",
    ">He saw the fine fat trout in the brook.\n",
    "\n",
    ">*The he the fine fat trout in the brook.\n",
    "\n",
    "As we will see in the next section, a grammar specifies how the sentence can be subdivided into its immediate constituents, and how these can be further subdivided until we reach the level of individual words\n",
    "\n",
    "> As we saw in 1, sentences can have arbitrary length. Consequently, phrase structure trees can have arbitrary depth. The cascaded chunk parsers we saw in 4 can only produce structures of bounded depth, so chunking methods aren't applicable here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context Free Grammar\n",
    "##### A Simple Grammar\n",
    "\n",
    "> **In NLTK, context-free grammars are defined in the nltk.grammar module. **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![parse_rdparsewindow.png](attachment:parse_rdparsewindow.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nltk.app.rdparser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our grammar licenses two trees for this sentence, the sentence is said to be structurally ambiguous. The ambiguity in question is called a prepositional phrase attachment ambiguity, as we saw earlier in this chapter. As you may recall, it is an ambiguity about attachment since the PP in the park needs to be attached to one of two places in the tree: either as a child of VP or else as a child of NP. When the PP is attached to  VP, the intended interpretation is that the seeing event happened in the park. However, if the  PP is attached to NP, then it was the man who was in the park, and the agent of the seeing (the dog) might have been sitting on the balcony of an apartment overlooking the park."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Writing Your Own Grammars\n",
    "Create and edit your grammar in a text file ex: 'gram.cfg'. You can the load it into NLTK and parse with it as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mC:\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('C:')\n  \u001b[0m\n  Searched in:\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-abb52169d6e9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'time'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mgrammar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'file:mygrammar.cfg'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0msent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'Mary saw Bob'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mrd_parser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRecursiveDescentParser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrammar\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtree\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[0;32m    832\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    833\u001b[0m     \u001b[1;31m# Load the resource.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 834\u001b[1;33m     \u001b[0mopened_resource\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    835\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    836\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'raw'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36m_open\u001b[1;34m(resource_url)\u001b[0m\n\u001b[0;32m    953\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'file'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    954\u001b[0m         \u001b[1;31m# urllib might not use mode='rb', so handle this one ourselves:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 955\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    956\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    957\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0murlopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    671\u001b[0m     \u001b[0msep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'*'\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m70\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    672\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 673\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    674\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    675\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mC:\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('C:')\n  \u001b[0m\n  Searched in:\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "grammar = nltk.data.load('file:mygrammar.cfg')\n",
    "sent = 'Mary saw Bob'.split()\n",
    "rd_parser = nltk.RecursiveDescentParser(grammar)\n",
    "for tree in rd.parser.parse(sent):\n",
    "    print(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Make sure that you put a .cfg suffix on the filename, and that there are no spaces in the string 'file:mygrammar.cfg'. If the command print(tree) produces no output, this is probably because your sentence sent is not admitted by your grammar. In this case, call the parser with tracing set to be on: rd_parser =\n",
    "nltk.RecursiveDescentParser(grammar1, trace=2). You can also check what productions are currently in the grammar with the command for p\n",
    "in grammar1.productions(): print(p)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**When you write CFGs for parsing in NLTK, you cannot combine grammatical categories with lexical items on the righthand side of the same production. Thus, a production such as PP -> 'of' NP is disallowed. In addition, you are not permitted to place multi-word lexical items on the righthand side of a production. So rather than writing NP -> 'New\n",
    "York', you have to resort to something like NP -> 'New_York' instead.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Recursion in Syntactic Structure\n",
    "\n",
    "Recursive if category occuring on the left hand side of a prodution also appears on the righthand side of production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: VP -> V NP PP will never be used\n"
     ]
    }
   ],
   "source": [
    "nltk.app.srparser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grammar1 = nltk.CFG.fromstring(\"\"\"\n",
    "S -> NP VP\n",
    "VP -> V NP | V NP PP\n",
    "PP -> P NP\n",
    "V -> 'saw' | 'ate' | 'walked'\n",
    "NP -> \"John\" | \"Mary\" | \"Bob\" | Det N | Det N PP\n",
    "Det -> \"a\" | \"an\" | \"the\" | \"my\"\n",
    "N -> \"man\" | \"dog\" | \"cat\" | \"telescope\" | \"park\"\n",
    "P -> \"in\" | \"on\" | \"by\" | \"with\"\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S (NP Mary) (VP (V saw) (NP (Det a) (N dog))))\n"
     ]
    }
   ],
   "source": [
    "sr_parser = nltk.ShiftReduceParser(grammar1)\n",
    "sent = 'Mary saw a dog'.split()\n",
    "for tree in sr_parser.parse(sent):\n",
    "    print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing 'Mary saw a dog'\n",
      "    [ * Mary saw a dog]\n",
      "  S [ 'Mary' * saw a dog]\n",
      "  R [ NP * saw a dog]\n",
      "  S [ NP 'saw' * a dog]\n",
      "  R [ NP V * a dog]\n",
      "  S [ NP V 'a' * dog]\n",
      "  R [ NP V Det * dog]\n",
      "  S [ NP V Det 'dog' * ]\n",
      "  R [ NP V Det N * ]\n",
      "  R [ NP V NP * ]\n",
      "  R [ NP VP * ]\n",
      "  R [ S * ]\n",
      "(S (NP Mary) (VP (V saw) (NP (Det a) (N dog))))\n"
     ]
    }
   ],
   "source": [
    "sr_parser = nltk.ShiftReduceParser(grammar1, trace =2)\n",
    "sent = 'Mary saw a dog'.split()\n",
    "for tree in sr_parser.parse(sent):\n",
    "    print(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Well Formed Substring Table\n",
    "\n",
    "For every word in text we can loop up in out grammar what category it belongs to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[V -> 'shot']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = ['I', 'shot', 'an', 'elephant', 'in', 'my', 'pajamas']\n",
    "gr_grammar.productions(rhs = text[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_wfst(tokens, grammar):\n",
    "    numtokens = len(tokens)\n",
    "    wfst = [[None for i in range(numtokens + 1)] for j in range(numtokens +1)]\n",
    "    for i in range(numtokens):\n",
    "        productions = grammar.productions(rhs = tokens[i])\n",
    "        wfst[i][i+1] = productions[0].lhs()\n",
    "    return wfst\n",
    "\n",
    "def complete_wfst(wfst, tokens, grammar, trace = False):\n",
    "    index = dict((p.rhs(), p.lhs()) for p in grammar.productions())\n",
    "    numtokens = len(tokens)\n",
    "    for span in range(2, numtokens + 1):\n",
    "        for start in range(numtokens + 1 - span):\n",
    "            end = start + span\n",
    "            for mid in range(start + 1, end):\n",
    "                nt1, nt2 = wfst[start][mid], wfst[mid][end]\n",
    "                if nt1 and nt2 and (nt1, nt2) in index:\n",
    "                    wfst[start][end] = index[(nt1, nt2)]\n",
    "                    if trace:\n",
    "                        print(\"[%s] %3s [%s] %3s [%s] ==> [%s] %3s [%s]\" % \\\n",
    "                        (start, nt1, mid, nt2, end, start, index[(nt1,nt2)], end))\n",
    "    return wfst\n",
    "\n",
    "def display(wfst, tokens):\n",
    "    print('\\nWFST ' + ' '.join((\"%-4d\" % i) for i in range(1, len(wfst))))\n",
    "    for i in range(len(wfst) - 1):\n",
    "        print(\"%d  \" %i, end = \" \")\n",
    "        for j in range(1, len(wfst)):\n",
    "            print(\"%-4s\"%(wfst[i][j] or '.'), end = \" \")\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WFST 1    2    3    4    5    6    7   \n",
      "0   NP   .    .    .    .    .    .    \n",
      "1   .    V    .    .    .    .    .    \n",
      "2   .    .    Det  .    .    .    .    \n",
      "3   .    .    .    N    .    .    .    \n",
      "4   .    .    .    .    P    .    .    \n",
      "5   .    .    .    .    .    Det  .    \n",
      "6   .    .    .    .    .    .    N    \n"
     ]
    }
   ],
   "source": [
    "tokens = 'I shot an elephant in my pajamas'.split()\n",
    "wfst0 = init_wfst(tokens, gr_grammar)\n",
    "display(wfst0, tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WFST 1    2    3    4    5    6    7   \n",
      "0   NP   .    .    S    .    .    S    \n",
      "1   .    V    .    VP   .    .    VP   \n",
      "2   .    .    Det  NP   .    .    .    \n",
      "3   .    .    .    N    .    .    .    \n",
      "4   .    .    .    .    P    .    PP   \n",
      "5   .    .    .    .    .    Det  NP   \n",
      "6   .    .    .    .    .    .    N    \n"
     ]
    }
   ],
   "source": [
    "wfst1 = complete_wfst(wfst0, tokens, gr_grammar)\n",
    "display(wfst1, tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2] Det [3]   N [4] ==> [2]  NP [4]\n",
      "[5] Det [6]   N [7] ==> [5]  NP [7]\n",
      "[1]   V [2]  NP [4] ==> [1]  VP [4]\n",
      "[4]   P [5]  NP [7] ==> [4]  PP [7]\n",
      "[0]  NP [1]  VP [4] ==> [0]   S [4]\n",
      "[1]  VP [4]  PP [7] ==> [1]  VP [7]\n",
      "[0]  NP [1]  VP [7] ==> [0]   S [7]\n",
      "\n",
      "WFST 1    2    3    4    5    6    7   \n",
      "0   NP   .    .    S    .    .    S    \n",
      "1   .    V    .    VP   .    .    VP   \n",
      "2   .    .    Det  NP   .    .    .    \n",
      "3   .    .    .    N    .    .    .    \n",
      "4   .    .    .    .    P    .    PP   \n",
      "5   .    .    .    .    .    Det  NP   \n",
      "6   .    .    .    .    .    .    N    \n"
     ]
    }
   ],
   "source": [
    "wfst1 = complete_wfst(wfst0, tokens, gr_grammar, trace = True)\n",
    "display(wfst1, tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grammar= (\n",
      "('    ', 'S -> NP VP,')\n",
      "('    ', 'VP -> VP PP,')\n",
      "('    ', 'VP -> V NP,')\n",
      "('    ', 'VP -> V,')\n",
      "('    ', 'NP -> Det N,')\n",
      "('    ', 'NP -> NP PP,')\n",
      "('    ', 'PP -> P NP,')\n",
      "('    ', \"NP -> 'John',\")\n",
      "('    ', \"NP -> 'I',\")\n",
      "('    ', \"Det -> 'the',\")\n",
      "('    ', \"Det -> 'my',\")\n",
      "('    ', \"Det -> 'a',\")\n",
      "('    ', \"N -> 'dog',\")\n",
      "('    ', \"N -> 'cookie',\")\n",
      "('    ', \"N -> 'table',\")\n",
      "('    ', \"N -> 'cake',\")\n",
      "('    ', \"N -> 'fork',\")\n",
      "('    ', \"V -> 'ate',\")\n",
      "('    ', \"V -> 'saw',\")\n",
      "('    ', \"P -> 'on',\")\n",
      "('    ', \"P -> 'under',\")\n",
      "('    ', \"P -> 'with',\")\n",
      ")\n",
      "tokens = ['John', 'ate', 'the', 'cake', 'on', 'the', 'table']\n",
      "Calling \"ChartParserApp(grammar, tokens)\"...\n"
     ]
    }
   ],
   "source": [
    "nltk.app.chartparser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies and Dependency Grammar\n",
    "\n",
    "Phrase structure grammar is concerned with how words and sequences of words combine to form constituents. Dependency grammar focusses on how words relate to other words. Dependency is a binary asymmetric relation that holds between a head and its dependents.\n",
    "\n",
    "The head of a sentence is usually taken to be the tensed verb and every other word is either dependent on the sentence head or connets to it through a path of dependencies.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dependency grammar with 7 productions\n",
      "  'shot' -> 'I'\n",
      "  'shot' -> 'elephant'\n",
      "  'shot' -> 'in'\n",
      "  'elephant' -> 'an'\n",
      "  'elephant' -> 'in'\n",
      "  'in' -> 'pajamas'\n",
      "  'pajamas' -> 'my'\n"
     ]
    }
   ],
   "source": [
    "dep_grammar = nltk.DependencyGrammar.fromstring(\"\"\"\n",
    "'shot' -> 'I' | 'elephant' | 'in'\n",
    "'elephant' -> 'an' | 'in'\n",
    "'in' -> 'pajamas'\n",
    "'pajamas' -> 'my'\n",
    "\"\"\")\n",
    "\n",
    "print(dep_grammar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A dependency graph is **projective** if, when all the words are written in linear order, the edges can be drawn above the words without crossing. This is equivalent to saying that a word and all its descendents (dependents and dependents of its dependents, etc.) form a contiguous sequence of words within the sentence. 5.1 is projective, and we can parse many sentences in English using a projective dependency parser. The next example shows how dep_grammar provides an alternative approach to capturing the attachment ambiguity that we examined earlier with phrase structure grammar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(shot I (elephant an (in (pajamas my))))\n",
      "(shot I (elephant an) (in (pajamas my)))\n"
     ]
    }
   ],
   "source": [
    "pdp = nltk.ProjectiveDependencyParser(dep_grammar)\n",
    "sent = 'I shot an elephant in my pajamas'.split()\n",
    "trees = pdp.parse(sent)\n",
    "for tree in trees:\n",
    "    print(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shot is the head "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for tree in trees:\n",
    "    print(tree.draw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Various criteria have been proposed for deciding what is the head H and what is the dependent D in a construction C. Some of the most important are the following:\n",
    "\n",
    "1. H determines the distribution class of C; or alternatively, the external syntactic properties of C are due to H.\n",
    "2. H determines the semantic type of C.\n",
    "3. H is obligatory while D may be optional.\n",
    "4. H selects D and determines whether it is obligatory or optional.\n",
    "5. The morphological form of D is determined by H (e.g. agreement or case government).\n",
    "\n",
    "When we say in a phrase structure grammar that the immediate constituents of a PP are P and NP, we are implicitly appealing to the head / dependent distinction. A prepositional phrase is a phrase whose head is a preposition; moreover, the NP is a dependent of P. The same distinction carries over to the other types of phrase that we have discussed. The key point to note here is that although phrase structure grammars seem very different from dependency grammars, they implicitly embody a recognition of dependency relations. While CFGs are not intended to directly capture dependencies, more recent linguistic frameworks have increasingly adopted formalisms which combine aspects of both approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Valency and the Lexicon\n",
    "Let us take a closer look at verbs and their dependents. The grammar in 3.3 correctly generates examples like (15d).\n",
    "\n",
    "\t\t\n",
    "a. The squirrel was frightened.\n",
    "\n",
    "b.\t\tChatterer saw the bear.\n",
    "\n",
    "c.\t\tChatterer thought Buster was angry.\n",
    "\n",
    "d.\t\tJoe put the fish on the log.\n",
    "\n",
    "\n",
    "These possibilities correspond to the following productions:\n",
    "\n",
    "Table 5.1:\n",
    "\n",
    "VP productions and their lexical heads\n",
    "\n",
    "VP -> V Adj\twas<br>\n",
    "VP -> V NP\tsaw<br>\n",
    "VP -> V S\tthought<br>\n",
    "VP -> V NP PP\tput<br>\n",
    "\n",
    "\n",
    "That is, was can occur with a following Adj, saw can occur with a following NP, thought can occur with a following S and put can occur with a following NP and PP. The dependents Adj, NP, PP and S are often called complements of the respective verbs and there are strong constraints on what verbs can occur with what complements. By contrast with (15d), the word sequences in (16d) are ill-formed:\n",
    "\n",
    "(16)\t\t\n",
    "a.\t\t*The squirrel was Buster was angry.\n",
    "\n",
    "b.\t\t*Chatterer saw frightened.\n",
    "\n",
    "c.\t\t*Chatterer thought the bear.\n",
    "\n",
    "d.\t\t*Joe put on the log.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "###  Scaling Up\n",
    "\n",
    "So far, we have only considered \"toy grammars,\" small grammars that illustrate the key aspects of parsing. But there is an obvious question as to whether the approach can be scaled up to cover large corpora of natural languages. How hard would it be to construct such a set of productions by hand? In general, the answer is: very hard. Even if we allow ourselves to use various formal devices that give much more succinct representations of grammar productions, it is still extremely difficult to keep control of the complex interactions between the many productions required to cover the major constructions of a language. **In other words, it is hard to modularize grammars so that one portion can be developed independently of the other parts. This in turn means that it is difficult to distribute the task of grammar writing across a team of linguists. Another difficulty is that as the grammar expands to cover a wider and wider range of constructions, there is a corresponding increase in the number of analyses which are admitted for any one sentence. In other words, ambiguity increases with coverage.**\n",
    "\n",
    "### Grammar Development\n",
    "\n",
    "Parsing build trees over sentences (phrase structure grammar). We have used grammar for small sentences and of small range. What to do if we want to scale up the coverage of our grammar which would be more realistic?\n",
    "\n",
    "##### Treebanks and Grammars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP-SBJ\n",
      "    (NP (NNP Pierre) (NNP Vinken))\n",
      "    (, ,)\n",
      "    (ADJP (NP (CD 61) (NNS years)) (JJ old))\n",
      "    (, ,))\n",
      "  (VP\n",
      "    (MD will)\n",
      "    (VP\n",
      "      (VB join)\n",
      "      (NP (DT the) (NN board))\n",
      "      (PP-CLR (IN as) (NP (DT a) (JJ nonexecutive) (NN director)))\n",
      "      (NP-TMP (NNP Nov.) (CD 29))))\n",
      "  (. .))\n"
     ]
    }
   ],
   "source": [
    "t = treebank.parsed_sents('wsj_0001.mrg')[0]\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def filter(tree):\n",
    "    \"\"\"Finds verbs that take sentential complements\"\"\"\n",
    "    child_nodes = [child.label() for child in tree\n",
    "                   if isinstance(child, nltk.Tree)]\n",
    "    return (tree.label() == 'VP') and ('S' in child_nodes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "#Searching a treebank to find sentential complements\n",
    "print([subtree for tree in treebank.parsed_sents() for subtree in tree.subtrees(filter)][10].draw())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%-below-level N: ['left'] V: ['be']\n",
      "%-from-year N: ['was'] V: ['declined', 'dropped', 'fell', 'grew', 'increased', 'plunged', 'rose', 'was']\n",
      "%-in-August N: ['was'] V: ['climbed', 'fell', 'leaping', 'rising', 'rose']\n",
      "%-in-September N: ['increased'] V: ['climbed', 'declined', 'dropped', 'edged', 'fell', 'grew', 'plunged', 'rose', 'slipped']\n",
      "%-in-week N: ['declined'] V: ['was']\n",
      "%-to-% N: ['add', 'added', 'backed', 'be', 'cut', 'go', 'grow', 'increased', 'increasing', 'is', 'offer', 'plummet', 'reduce', 'rejected', 'rise', 'risen', 'shaved', 'wants', 'yield', 'zapping'] V: ['fell', 'rise', 'slipped']\n",
      "%-to-million N: ['declining'] V: ['advanced', 'climbed', 'cutting', 'declined', 'declining', 'dived', 'dropped', 'edged', 'fell', 'gained', 'grew', 'increased', 'jump', 'jumped', 'plunged', 'rising', 'rose', 'slid', 'slipped', 'soared', 'tumbled']\n",
      "1-to-21 N: ['dropped'] V: ['dropped']\n",
      "1-to-33 N: ['gained'] V: ['dropped', 'fell', 'jumped']\n",
      "1-to-4 N: ['added'] V: ['gained']\n",
      "1-to-47 N: ['jumped'] V: ['added', 'rose']\n",
      "1-to-point N: ['ended'] V: ['fell', 'rose']\n",
      "3-to-17 N: ['lost'] V: ['lost']\n",
      "500,000-in-fines N: ['paid'] V: ['paid']\n",
      "6.9-on-scale N: ['registered'] V: ['registered']\n",
      "access-to-AZT N: ['had'] V: ['had']\n",
      "access-to-arena N: ['permits'] V: ['lack']\n",
      "activity-in-part N: ['showed'] V: ['attributed']\n",
      "agreement-in-principle N: ['reached'] V: ['reached']\n",
      "agreement-with-Inc. N: ['announced', 'signed'] V: ['signed']\n",
      "agreement-with-creditors N: ['reached'] V: ['nearing']\n",
      "agreement-with-regulators N: ['presages', 'reach'] V: ['reach']\n",
      "aid-to-Contras N: ['renewing'] V: ['renewing']\n",
      "alliance-with-GM N: ['discussing', 'wrapping'] V: ['forge', 'have', 'negotiating']\n",
      "approval-for-drug N: ['granted'] V: ['obtain']\n",
      "attention-to-comments N: ['paid'] V: ['paid']\n",
      "attention-to-concerns N: ['pay'] V: ['show']\n",
      "attention-to-reports N: ['paid'] V: ['pay']\n",
      "bid-for-company N: ['fend', 'launch'] V: ['made', 'make']\n",
      "bid-for-million N: ['finance'] V: ['had']\n",
      "bids-for-company N: ['submitted'] V: ['solicit']\n",
      "billion-in-cash N: ['pay', 'raise'] V: ['raise']\n",
      "billion-of-bills N: ['sell', 'sold'] V: ['sold']\n",
      "billion-over-years N: ['total'] V: ['spent']\n",
      "billion-to-billion N: ['cause', 'place'] V: ['increased', 'rose']\n",
      "business-to-firms N: ['cutting'] V: ['give', 'transfer']\n",
      "business-with-them N: ['cease'] V: ['do']\n",
      "cap-on-amount N: ['eliminate'] V: ['places']\n",
      "cents-to-cents N: ['be', 'recovering'] V: ['fell', 'rose']\n",
      "change-in-earnings N: ['had'] V: ['had']\n",
      "changes-for-% N: ['measures'] V: ['measures', 'monitors']\n",
      "charge-in-quarter N: ['took'] V: ['had', 'included', 'incur', 'take', 'took']\n",
      "collar-on-trading N: ['re-establishing'] V: ['reinstating']\n",
      "commitments-from-banks N: ['secured', 'won'] V: ['obtained']\n",
      "competition-from-competitors N: ['faced'] V: ['fend']\n",
      "competition-in-production N: ['reduce'] V: ['reduce']\n",
      "contract-for-parts N: ['awarded', 'given', 'won'] V: ['received']\n",
      "contract-for-support N: ['awarded', 'issued'] V: ['received']\n",
      "contract-from-Co. N: ['received'] V: ['won']\n",
      "contract-with-Warner N: ['violates'] V: ['terminate']\n",
      "control-of-Inc. N: ['took'] V: ['seek']\n",
      "decline-for-quarter N: ['posted'] V: ['reported']\n",
      "decline-in-August N: ['followed', 'following', 'recorded'] V: ['following']\n",
      "decline-in-earnings N: ['alleviate', 'report', 'reported'] V: ['expects']\n",
      "declines-in-prices N: ['reflect'] V: ['had']\n",
      "disputes-with-company N: ['resolve'] V: ['resolve']\n",
      "domestic-production-through-July N: ['includes'] V: ['includes']\n",
      "drop-in-earnings N: ['posted'] V: ['posted']\n",
      "drop-in-profit N: ['experienced', 'had', 'posted', 'reported', 'reporting'] V: ['posted']\n",
      "earnings-for-companies N: ['reported'] V: ['reported']\n",
      "earnings-for-quarter N: ['posting'] V: ['posted', 'report', 'reported']\n",
      "earnings-in-quarter N: ['projecting'] V: ['had']\n",
      "earnings-of-million N: ['had', 'include', 'posted', 'reported'] V: ['reported']\n",
      "effect-on-market N: ['had'] V: ['had']\n",
      "emphasis-on-quality N: ['be'] V: ['place']\n",
      "financing-for-buy-out N: ['deliver', 'get'] V: ['obtaining']\n",
      "floor-for-price N: ['establishes'] V: ['providing']\n",
      "foot-in-door N: ['wanted'] V: ['getting']\n",
      "funding-for-abortion N: ['supporting'] V: ['oppose']\n",
      "funds-for-station N: ['including', 'providing'] V: ['includes']\n",
      "gain-from-sale N: ['included', 'includes'] V: ['a-Includes', 'including', 'record', 'report']\n",
      "gain-in-profit N: ['posted', 'reported'] V: ['posted']\n",
      "head-to-head N: ['going'] V: ['go']\n",
      "impact-on-market N: ['have'] V: ['has', 'have']\n",
      "impact-on-markets N: ['had'] V: ['have']\n",
      "impact-on-results N: ['have'] V: ['have']\n",
      "income-for-quarter N: ['announcing'] V: ['report']\n",
      "increase-in-earnings N: ['reported'] V: ['posted']\n",
      "information-from-companies N: ['requested'] V: ['steal']\n",
      "inquiry-into-activities N: ['conducted'] V: ['drop']\n",
      "interest-in-company N: ['bought', 'have', 'holds', 'owning', 'retain'] V: ['represent']\n",
      "interest-in-metals N: ['create'] V: ['was']\n",
      "interest-on-loans N: ['computing'] V: ['pay']\n",
      "loans-to-China N: ['suspended'] V: ['resuming']\n",
      "loss-for-quarter N: ['announced', 'have', 'post', 'posted', 'reported', 'reporting'] V: ['post', 'report', 'reported']\n",
      "loss-in-quarter N: ['expect', 'had'] V: ['caused', 'had', 'posted', 'took']\n",
      "losses-in-years N: ['reported'] V: ['had']\n",
      "markets-in-stocks N: ['making'] V: ['make']\n",
      "million-before-tax N: ['reported'] V: ['contributed']\n",
      "million-for-initiative N: ['attached'] V: ['add']\n",
      "million-for-stake N: ['pay'] V: ['paid', 'pay', 'putting']\n",
      "million-from-funds N: ['commit'] V: ['raises']\n",
      "million-from-operations N: ['included'] V: ['reported']\n",
      "million-from-sale N: ['including'] V: ['take']\n",
      "million-in-payments N: ['make', 'owes', 'pay', 'receive'] V: ['fallen']\n",
      "million-of-debt N: ['add', 'borrow', 'consolidate', 'convert', 'downgraded', 'includes', 'pay', 'raise'] V: ['assume']\n",
      "million-on-revenue N: ['earned'] V: ['earned', 'was', 'were']\n",
      "million-on-sales N: ['earned'] V: ['earned', 'reach', 'totaled', 'was', 'were']\n",
      "million-to-million N: ['be', 'bills', 'cost', 'pump', 'sell', 'totaled'] V: ['declined', 'fell', 'spend', 'tumbled']\n",
      "month-in-time N: ['delivered'] V: ['delivered']\n",
      "net-on-revenue N: ['posted'] V: ['reported']\n",
      "nothing-about-it N: ['knew'] V: ['doing']\n",
      "offer-for-all N: ['begun', 'make'] V: ['begin']\n",
      "offer-for-shares N: ['began', 'extended'] V: ['launched', 'made']\n",
      "offer-for-stock N: ['extended'] V: ['make']\n",
      "offer-from-group N: ['rejected'] V: ['received']\n",
      "office-in-Worth N: ['Call'] V: ['has']\n",
      "pace-with-inflation N: ['keep', 'keeping'] V: ['keep']\n",
      "payment-on-million N: ['make'] V: ['make']\n",
      "payments-on-debt N: ['cover', 'make'] V: ['make']\n",
      "president-in-charge N: ['is', 'named'] V: ['been']\n",
      "pressure-on-government N: ['keep'] V: ['keep', 'put']\n",
      "pressure-on-prices N: ['suffered'] V: ['keep', 'put']\n",
      "price-for-incentives N: ['paid'] V: ['paid']\n",
      "prices-on-market N: ['push'] V: ['bring']\n",
      "profit-for-year N: ['report'] V: ['report']\n",
      "profit-from-discontinued N: ['had'] V: ['was']\n",
      "profit-in-quarter N: ['indicates'] V: ['produce', 'recorded']\n",
      "projections-for-year N: ['slashed'] V: ['exceed']\n",
      "provisions-for-loans N: ['taken'] V: ['made']\n",
      "rates-to-% N: ['boosting'] V: ['increase', 'pushed', 'raised']\n",
      "reporter-in-bureau N: ['is'] V: ['is']\n",
      "restrictions-on-use N: ['waiving'] V: ['impose']\n",
      "revenue-for-year N: ['projected'] V: ['had']\n",
      "revenue-in-quarter N: ['expects'] V: ['had']\n",
      "sales-in-excess N: ['combined'] V: ['had']\n",
      "sales-in-quarter N: ['had'] V: ['increasing']\n",
      "sales-of-million N: ['expected', 'had', 'has', 'have', 'posted'] V: ['had']\n",
      "salvo-in-battle N: ['marking'] V: ['marking']\n",
      "services-for-customers N: ['offering'] V: ['provide']\n",
      "shareholder-in-bank N: ['become'] V: ['become']\n",
      "stake-in-Airlines N: ['acquiring', 'buy', 'raise'] V: ['buy']\n",
      "stake-in-Mixte N: ['bring'] V: ['boost']\n",
      "stake-in-Rally N: ['hold'] V: ['had']\n",
      "stake-in-company N: ['bought', 'building', 'built', 'buying', 'give', 'hold', 'obtaining', 'own', 'owns', 'raised', 'take'] V: ['accumulating', 'had', 'has', 'holds', 'own']\n",
      "stake-in-concern N: ['acquires', 'lowered'] V: ['retaining']\n",
      "stake-in-unit N: ['sell'] V: ['acquire']\n",
      "stake-in-venture N: ['has', 'hold', 'holds'] V: ['held']\n",
      "suit-against-Keating N: ['press'] V: ['brought']\n",
      "swings-in-market N: ['cause', 'create'] V: ['cause']\n",
      "system-for-city N: ['design'] V: ['design']\n",
      "system-in-Pakistan N: ['operate'] V: ['operate']\n",
      "time-for-Congress N: ['is'] V: ['buy', 'buys']\n",
      "venture-with-Co. N: ['started'] V: ['started']\n",
      "ventures-with-companies N: ['established'] V: ['form']\n",
      "verdict-in-case N: ['is', 'won'] V: ['won']\n",
      "volatility-in-stocks N: ['ignoring'] V: ['see']\n",
      "vote-on-issue N: ['allow'] V: ['prevent']\n",
      "way-for-declines N: ['open'] V: ['pave']\n",
      "writer-in-York N: ['is'] V: ['is']\n",
      "yen-to-yen N: ['shed'] V: ['advanced', 'fell', 'gained', 'lost', 'rose']\n"
     ]
    }
   ],
   "source": [
    "entries = ppattach.attachments('training')\n",
    "table = defaultdict(lambda: defaultdict(set))\n",
    "for entry in entries:\n",
    "    key = entry.noun1 + '-' +entry.prep + '-' +entry.noun2\n",
    "    table[key][entry.attachment].add(entry.verb)\n",
    "    \n",
    "for key in sorted(table):\n",
    "     if len(table[key]) > 1:\n",
    "         print(key, 'N:', sorted(table[key]['N']), 'V:', sorted(table[key]['V']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PPAttachment(sent='0', verb='join', noun1='board', prep='as', noun2='director', attachment='V'), PPAttachment(sent='1', verb='is', noun1='chairman', prep='of', noun2='N.V.', attachment='N'), PPAttachment(sent='2', verb='named', noun1='director', prep='of', noun2='conglomerate', attachment='N'), PPAttachment(sent='3', verb='caused', noun1='percentage', prep='of', noun2='deaths', attachment='N'), PPAttachment(sent='5', verb='using', noun1='crocidolite', prep='in', noun2='filters', attachment='V')]\n"
     ]
    }
   ],
   "source": [
    "print(entries[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amongst the output lines of this program we find offer-from-group N: ['rejected'] V: ['received'], which indicates that received expects a separate PP complement attached to the VP, while rejected does not. As before, we can use this information to help construct the grammar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-75-9154956624fc>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-75-9154956624fc>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    python -m nltk.downloader large_grammar\u001b[0m\n\u001b[1;37m                 ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "python -m nltk.downloader large_grammar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pernicious Ambiguity\n",
    "\n",
    "**Pernicious:** Having an harmful especially in a subtle manner\n",
    "\n",
    "As mentioned before, as the coverage of grammar increases and the length of the sentence grows, the number of parse trees grows rapidly(ambiguity increases).\n",
    "\n",
    "Let's explore this issue with the help of a simple example. The word fish is both a noun and a verb. We can make up the sentence fish fish fish, meaning fish like to fish for other fish. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grammar = nltk.CFG.fromstring(\"\"\"\n",
    "S -> NP V NP\n",
    "NP -> NP Sbar\n",
    "Sbar -> NP V\n",
    "NP -> 'fish'\n",
    "V -> 'fish'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can try parsing a longer sentence, fish fish fish fish fish, which amongst other things, means 'fish that other fish fish are in the habit of fishing fish themselves'. We use the NLTK chart parser, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S (NP fish) (V fish) (NP (NP fish) (Sbar (NP fish) (V fish))))\n",
      "(S (NP (NP fish) (Sbar (NP fish) (V fish))) (V fish) (NP fish))\n"
     ]
    }
   ],
   "source": [
    "tokens = ['fish'] * 5\n",
    "cp =nltk.ChartParser(grammar)\n",
    "for tree in cp.parse(tokens):\n",
    "    print(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the length of this sentence goes up (3, 5, 7, ...) we get the following numbers of parse trees: 1; 2; 5; 14; 42; 132; 429; 1,430; 4,862; 16,796; 58,786; 208,012; ... (These are the Catalan numbers, which we saw in an exercise in 4). The last of these is for a sentence of length 23, the average length of sentences in the WSJ section of Penn Treebank. For a sentence of length 50 there would be over 1012 parses, and this is only half the length of the Piglet sentence (1), which young children process effortlessly. No practical NLP system could construct millions of trees for a sentence and choose the appropriate one in the context. It's clear that humans don't do this either!\n",
    "\n",
    "Note that the problem is not with our choice of example. (Church & Patil, 1982) point out that the syntactic ambiguity of PP attachment in sentences like (18) also grows in proportion to the Catalan numbers.\n",
    "\n",
    "(18)\t\tPut the block in the box on the table.\n",
    "\n",
    "So much for structural ambiguity; what about lexical ambiguity? As soon as we try to construct a broad-coverage grammar, we are forced to make lexical entries highly ambiguous for their part of speech. In a toy grammar, a is only a determiner, dog is only a noun, and runs is only a verb. However, in a broad-coverage grammar, a is also a noun (e.g. part a), dog is also a verb (meaning to follow closely), and runs is also a noun (e.g. ski runs). In fact, all words can be referred to by name: e.g. the verb 'ate' is spelled with three letters; in speech we do not need to supply quotation marks. Furthermore, it is possible to verb most nouns. Thus a parser for a broad-coverage grammar will be overwhelmed with ambiguity. Even complete gibberish will often have a reading, e.g. the a are of I. As (Klavans & Resnik, 1996) has pointed out, this is not word salad but a grammatical noun phrase, in which are is a noun meaning a hundredth of a hectare (or 100 sq m), and a and I are nouns designating coordinates, \n",
    "\n",
    "The solution to these problems is provided by probabilistic parsing, which allows us to rank the parses of an ambiguous sentence on the basis of evidence from corpora."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighted Grammar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def give(t):\n",
    "    return t.label == 'VP' and len(t) > 2 and t[1].label() == 'NP'\\\n",
    "    and (t[2].label()=='PP-DTV' or t[2]/label() == 'NP')\\\n",
    "    and ('give' in t[0].leaves() or 'gave' in t[0].leaves())\n",
    "    \n",
    "def sent(t):\n",
    "    return ' '.join(token for token in t.leaves() if token[0] not in '*-0')\n",
    "\n",
    "def print_node(t, width):\n",
    "    output = '%s %s: %s / %s: %s' % (sent(t[0]), t[1].label(),\n",
    "                                    sent(t[1]), t[2].label(), sent(t[2]))\n",
    "    if len(output) > width:\n",
    "        output = output[width] + \"...\"\n",
    "        print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for tree in treebank.parsed_sents():\n",
    "    for t in tree.subtrees(give):\n",
    "        print_node(t, 72)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A probabilistic context free grammar (or PCFG)** is a context free grammar that associates a probability with each of its productions. It generates the same set of parses for a text that the corresponding context free grammar does, and assigns a probability to each parse. The probability of a parse generated by a PCFG is simply the product of the probabilities of the productions used to generate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grammar = nltk.PCFG.fromstring(\"\"\"\n",
    "S -> NP VP        [1.0]\n",
    "VP -> TV NP       [0.4]\n",
    "VP -> IV          [0.3]\n",
    "VP -> DATV NP NP  [0.3]\n",
    "TV -> 'saw'       [1.0]\n",
    "IV -> 'ate'       [1.0]\n",
    "DATV -> 'gave'    [1.0]\n",
    "NP -> 'telescopes'[0.8]\n",
    "NP -> 'Jack'      [0.2]\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grammar with 9 productions (start state = S)\n",
      "    S -> NP VP [1.0]\n",
      "    VP -> TV NP [0.4]\n",
      "    VP -> IV [0.3]\n",
      "    VP -> DATV NP NP [0.3]\n",
      "    TV -> 'saw' [1.0]\n",
      "    IV -> 'ate' [1.0]\n",
      "    DATV -> 'gave' [1.0]\n",
      "    NP -> 'telescopes' [0.8]\n",
      "    NP -> 'Jack' [0.2]\n"
     ]
    }
   ],
   "source": [
    "print(grammar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " In order to ensure that the trees generated by the grammar form a probability distribution, PCFG grammars impose the constraint that all productions with a given left-hand side must have probabilities that sum to one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "viterbi_parser = nltk.ViterbiParser(grammar)\n",
    "for tree in viterbi_parser.parse(['Jack', 'saw', 'telescopes']):\n",
    "    print(tree.draw())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that parse trees are assigned probabilities, it no longer matters that there may be a huge number of possible parses for a given sentence. A parser will be responsible for finding the most likely parses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "example = \"Jack saw a telescope\".split()\n",
    "tags = nltk.pos_tag(example)\n",
    "chunk = nltk.ne_chunk(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chunk.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Jack', 'NNP'), ('saw', 'VBD'), ('a', 'DT'), ('telescope', 'NN')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "example = \"I cut my Knee\".split()\n",
    "tags = nltk.pos_tag(example)\n",
    "chunk = nltk.ne_chunk(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 'PRP'), ('cut', 'VBD'), ('my', 'PRP$'), ('Knee', 'NNP')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chunk.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "example = \"I have a cut on my knee\".split()\n",
    "tags = nltk.pos_tag(example)\n",
    "chunk = nltk.ne_chunk(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 'PRP'),\n",
       " ('have', 'VBP'),\n",
       " ('a', 'DT'),\n",
       " ('cut', 'NN'),\n",
       " ('on', 'IN'),\n",
       " ('my', 'PRP$'),\n",
       " ('knee', 'NN')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "example = \"Show me the weather\".split()\n",
    "tags = nltk.pos_tag(example)\n",
    "chunk = nltk.ne_chunk(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Show', 'VB'), ('me', 'PRP'), ('the', 'DT'), ('weather', 'NN')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chunk.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
