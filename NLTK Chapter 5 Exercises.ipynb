{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 5 Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2697.3"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown_tagged_sents = brown.tagged_sents(categories = 'editorial')\n",
    "brown_sents = brown.sents(categories = 'editorial')\n",
    "size = len(brown_tagged_sents) * 0.9\n",
    "size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sents = brown_tagged_sents[:int(size)]\n",
    "test_sents = brown_tagged_sents[int(size):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Tokenize and tag the following sentence: _'They wind back the clock, while we chase after the wind'_. What different pronunciations and parts of speech are involved?**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('They', 'PRP'),\n",
       " ('wind', 'VBP'),\n",
       " ('back', 'RB'),\n",
       " ('the', 'DT'),\n",
       " ('clock', 'NN'),\n",
       " (',', ','),\n",
       " ('while', 'IN'),\n",
       " ('we', 'PRP'),\n",
       " ('chase', 'VBP'),\n",
       " ('after', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('wind', 'NN')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string = 'They wind back the clock, while we chase after the wind'\n",
    "tokens = nltk.word_tokenize(string)\n",
    "nltk.pos_tag(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{',', 'DT', 'IN', 'NN', 'PRP', 'RB', 'VBP'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(tag for word, tag in nltk.pos_tag(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**4. Review the mappings in 3.1. Discuss any other examples of mappings you can think of. What type of information do they map from and to?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. List maps from its index to the element\n",
    "2. Contacts page letter to the names\n",
    "3. Search Engine keywords to webpages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Using the Python interpreter in interactive mode, experiment with the dictionary examples in this chapter. Create a dictionary d, and add some entries. What happens if you try to access a non-existent entry, e.g. d['xyz']?** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'xyz'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-4f582fd48a72>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#You should get an error if you don't use defaultdict from Collections\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0md\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m'a'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m'b'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0md\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'xyz'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m: 'xyz'"
     ]
    }
   ],
   "source": [
    "#You should get an error if you don't use defaultdict from Collections\n",
    "d = {1:'a', 2:'b', 3:'c'}\n",
    "d['xyz']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "d = defaultdict(str)\n",
    "d[1] = 'a'\n",
    "d[2] = 'b'\n",
    "d[3] ='c'\n",
    "d['xyz']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. Try deleting an element from a dictionary d, using the syntax del d['abc']. Check that the item was deleted.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del d[1]\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7. Create two dictionaries, d1 and d2, and add some entries to each. Now issue the command d1.update(d2). What did this do? What might it be useful for?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1  = {1:'a', 2:'b', 3:'c'}\n",
    "d2 = {4: 'd', 5: 'e', 6: 'f'}\n",
    "d1.update(d2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Updates d1 with all elements of d2. When you want to combine dictionaries with different entries in each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d1.update(d2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1[7] = 'g'\n",
    "print(d1)\n",
    "d2[7] = 'h'\n",
    "d1.update(d2)\n",
    "d1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**11.  Learn about the affix tagger (type help(nltk.AffixTagger)). Train an affix tagger and run it on some new text. Experiment with different settings for the affix length and the minimum word length. Discuss your findings.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class AffixTagger in module nltk.tag.sequential:\n",
      "\n",
      "class AffixTagger(ContextTagger)\n",
      " |  A tagger that chooses a token's tag based on a leading or trailing\n",
      " |  substring of its word string.  (It is important to note that these\n",
      " |  substrings are not necessarily \"true\" morphological affixes).  In\n",
      " |  particular, a fixed-length substring of the word is looked up in a\n",
      " |  table, and the corresponding tag is returned.  Affix taggers are\n",
      " |  typically constructed by training them on a tagged corpus.\n",
      " |  \n",
      " |  Construct a new affix tagger.\n",
      " |  \n",
      " |  :param affix_length: The length of the affixes that should be\n",
      " |      considered during training and tagging.  Use negative\n",
      " |      numbers for suffixes.\n",
      " |  :param min_stem_length: Any words whose length is less than\n",
      " |      min_stem_length+abs(affix_length) will be assigned a\n",
      " |      tag of None by this tagger.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      AffixTagger\n",
      " |      ContextTagger\n",
      " |      SequentialBackoffTagger\n",
      " |      nltk.tag.api.TaggerI\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, train=None, model=None, affix_length=-3, min_stem_length=2, backoff=None, cutoff=0, verbose=False)\n",
      " |      :param context_to_tag: A dictionary mapping contexts to tags.\n",
      " |      :param backoff: The backoff tagger that should be used for this tagger.\n",
      " |  \n",
      " |  context(self, tokens, index, history)\n",
      " |      :return: the context that should be used to look up the tag\n",
      " |          for the specified token; or None if the specified token\n",
      " |          should not be handled by this tagger.\n",
      " |      :rtype: (hashable)\n",
      " |  \n",
      " |  encode_json_obj(self)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  decode_json_obj(obj) from abc.ABCMeta\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  json_tag = 'nltk.tag.sequential.AffixTagger'\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from ContextTagger:\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __unicode__ = __str__(self, /)\n",
      " |      Return str(self).\n",
      " |  \n",
      " |  choose_tag(self, tokens, index, history)\n",
      " |      Decide which tag should be used for the specified token, and\n",
      " |      return that tag.  If this tagger is unable to determine a tag\n",
      " |      for the specified token, return None -- do not consult\n",
      " |      the backoff tagger.  This method should be overridden by\n",
      " |      subclasses of SequentialBackoffTagger.\n",
      " |      \n",
      " |      :rtype: str\n",
      " |      :type tokens: list\n",
      " |      :param tokens: The list of words that are being tagged.\n",
      " |      :type index: int\n",
      " |      :param index: The index of the word whose tag should be\n",
      " |          returned.\n",
      " |      :type history: list(str)\n",
      " |      :param history: A list of the tags for all words before *index*.\n",
      " |  \n",
      " |  size(self)\n",
      " |      :return: The number of entries in the table used by this\n",
      " |          tagger to map from contexts to tags.\n",
      " |  \n",
      " |  unicode_repr = __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from SequentialBackoffTagger:\n",
      " |  \n",
      " |  tag(self, tokens)\n",
      " |      Determine the most appropriate tag sequence for the given\n",
      " |      token sequence, and return a corresponding list of tagged\n",
      " |      tokens.  A tagged token is encoded as a tuple ``(token, tag)``.\n",
      " |      \n",
      " |      :rtype: list(tuple(str, str))\n",
      " |  \n",
      " |  tag_one(self, tokens, index, history)\n",
      " |      Determine an appropriate tag for the specified token, and\n",
      " |      return that tag.  If this tagger is unable to determine a tag\n",
      " |      for the specified token, then its backoff tagger is consulted.\n",
      " |      \n",
      " |      :rtype: str\n",
      " |      :type tokens: list\n",
      " |      :param tokens: The list of words that are being tagged.\n",
      " |      :type index: int\n",
      " |      :param index: The index of the word whose tag should be\n",
      " |          returned.\n",
      " |      :type history: list(str)\n",
      " |      :param history: A list of the tags for all words before *index*.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from SequentialBackoffTagger:\n",
      " |  \n",
      " |  backoff\n",
      " |      The backoff tagger for this tagger.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from nltk.tag.api.TaggerI:\n",
      " |  \n",
      " |  evaluate(self, gold)\n",
      " |      Score the accuracy of the tagger against the gold standard.\n",
      " |      Strip the tags from the gold standard text, retag it using\n",
      " |      the tagger, then compute the accuracy score.\n",
      " |      \n",
      " |      :type gold: list(list(tuple(str, str)))\n",
      " |      :param gold: The list of tagged sentences to score the tagger on.\n",
      " |      :rtype: float\n",
      " |  \n",
      " |  tag_sents(self, sentences)\n",
      " |      Apply ``self.tag()`` to each element of *sentences*.  I.e.:\n",
      " |      \n",
      " |          return [self.tag(sent) for sent in sentences]\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from nltk.tag.api.TaggerI:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(nltk.AffixTagger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('There', 'EX'),\n",
       " ('ought', 'NN'),\n",
       " ('to', None),\n",
       " ('be', None),\n",
       " ('a', None),\n",
       " ('point', 'NN'),\n",
       " ('beyond', 'OD'),\n",
       " ('which', 'WDT'),\n",
       " ('we', None),\n",
       " ('will', None),\n",
       " ('not', None),\n",
       " ('allow', 'VB'),\n",
       " ('ourselves', 'VBZ'),\n",
       " ('to', None),\n",
       " ('go', None),\n",
       " ('regardless', 'NN'),\n",
       " ('of', None),\n",
       " ('what', None),\n",
       " ('Russia', 'NP'),\n",
       " ('does', None),\n",
       " ('.', None)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "affix_tagger = nltk.AffixTagger(train_sents)\n",
    "affix_tagger.tag(brown_sents[2400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23608527686364286"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "affix_tagger.evaluate(test_sents) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
