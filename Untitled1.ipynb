{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing Sentence Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk, pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some Grammatical Dilemmas\n",
    "##### Linguistic Data and Unlimited Possibilities\n",
    "\n",
    "We can concoct new sentences, one which has never been used in a language, which can understood by all speakers of the language. Sentences have a structure which can allow them to be extended, indefinitely. Sentences can be embedded inside larger sentences. (The last two sentences are actually indentical).\n",
    "\n",
    "These structures are represented by a grammar. The purpose of a grammar is to give an explicit description of language. Is grammar largely finite set of oserved utterances and written texts? \n",
    "\n",
    "We're goonna consider 'language' to be an enormous collection of all grammatical sentences and a grammer is a formal notation that can be used for 'generating' the members of this set. Grammars use recursive productions of the form S -> S and S\n",
    "\n",
    "##### Ubiquitous Ambiguity\n",
    "\n",
    "> While hunting in Africa, **I shot an elephant in my pajamas**. How he got into my pajamas, I don't know."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "gr_grammar = nltk.CFG.fromstring(\"\"\"\n",
    "S -> NP VP\n",
    "PP -> P NP\n",
    "NP -> Det N | Det N PP | 'I'\n",
    "VP -> V NP | VP PP\n",
    "Det -> 'an' | 'my'\n",
    "N -> 'elephant' | 'pajamas'\n",
    "V -> 'shot'\n",
    "P -> 'in'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP I)\n",
      "  (VP\n",
      "    (VP (V shot) (NP (Det an) (N elephant)))\n",
      "    (PP (P in) (NP (Det my) (N pajamas)))))\n",
      "(S\n",
      "  (NP I)\n",
      "  (VP\n",
      "    (V shot)\n",
      "    (NP (Det an) (N elephant) (PP (P in) (NP (Det my) (N pajamas))))))\n"
     ]
    }
   ],
   "source": [
    "sent = ['I', 'shot', 'an', 'elephant', 'in', 'my', 'pajamas']\n",
    "\n",
    "parser = nltk.ChartParser(gr_grammar)\n",
    "for tree in parser.parse(sent):\n",
    "    print(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preposition**: a word governing, and usually preceding, a noun or pronoun and expressing a relation to another word or element in the clause, as in ‘the man on the platform’, ‘she arrived after dinner’, ‘what did you do it for ?’.\n",
    "\n",
    "There's no ambiguity in the meaning of any word. For example, 'shot' only refers to the act of using a gun but not taking a picture from the camera.\n",
    "\n",
    "Notice there are two grammars above . One which takes the first rule of VP and other uses the second.\n",
    "\n",
    "### What's the Use of Syntax?\n",
    "##### Beyond the n-grams\n",
    "\n",
    "We can use frequency information in bigrams to generate text that seems perfectly acceptable for small sequences of words but rapidly degenerated into nonsense. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "living creature that he said , and the land of the land of the land "
     ]
    }
   ],
   "source": [
    "text = nltk.corpus.genesis.words('english-kjv.txt')\n",
    "bigrams = nltk.bigrams(text)\n",
    "cfd = nltk.ConditionalFreqDist(bigrams)\n",
    "\n",
    "def generate_words(cfd, word, num =15):\n",
    "    for i in range(num):\n",
    "        print(word, end = ' ')\n",
    "        word = cfd[word].max()\n",
    "\n",
    "generate_words(cfd, 'living')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "love , and the land of the land of the land of the land of "
     ]
    }
   ],
   "source": [
    "generate_words(cfd, 'love')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examples : \n",
    "1. He roared with me the pail slip down his back.\n",
    "2. The worst part and tclumsy looking for whoever heard light\n",
    "\n",
    "You intuitively understand that these sequences are 'word-salad' ( in other words, shit) but yu find it hard to pind down what's wrong with them. \n",
    "So one benefit of studying grammar is that it provides a conceptual framework and vocabulary for spelling out these intuitions. \n",
    "\n",
    "*the worst part and clumsy looking* looks like a **coordinate structure**, where two phrases are joinf by a coordination conjuction such as *and, but* or *or*\n",
    "\n",
    "Coordinate Structure:\n",
    "\n",
    "if **v1** and **v2** are both phrases of grmaatical category X, then **v1 and v2** is also a phrase of category X\n",
    "\n",
    ">a.\t\tThe book's ending was (NP the worst part and the best part) for me.\n",
    "\n",
    ">b.\t\tOn land they are (AP slow and clumsy looking).\n",
    "\n",
    "In the first, two NPs (noun phrases) have been conjoined to make an NP, while in the second, two APs (adjective phrases) have been conjoined to make an AP.\n",
    " \n",
    "> What we can't do is conjoin an NP and an AP, which is why *the worst parst* and clumsy looking* is ungrammatical.\n",
    "\n",
    "Before we can formalize these ideas, we need to understand the concept of **constituent structure**.\n",
    "\n",
    "Constituent structure is based on the observation that words combine with other words to form units. The evidence that a sequence of words forms such a unit is given subsitutability - that is , a sequence of words in a well-formaed sentence can be replaced by a shorted sequence without rendering the sentence ill-formed or changing its meaning. \n",
    "\n",
    "> The little bear saw the fine fat trout in the brook.\n",
    "\n",
    "The fact that we can substitute He for *The little bear* indicates that the latter sequence is a unit. By contrast, we cannot replace *little bear saw* in the same way.\n",
    "\n",
    ">He saw the fine fat trout in the brook.\n",
    "\n",
    ">*The he the fine fat trout in the brook.\n",
    "\n",
    "As we will see in the next section, a grammar specifies how the sentence can be subdivided into its immediate constituents, and how these can be further subdivided until we reach the level of individual words\n",
    "\n",
    "> As we saw in 1, sentences can have arbitrary length. Consequently, phrase structure trees can have arbitrary depth. The cascaded chunk parsers we saw in 4 can only produce structures of bounded depth, so chunking methods aren't applicable here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context Free Grammar\n",
    "##### A Simple Grammar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![parse_rdparsewindow.png](attachment:parse_rdparsewindow.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
