{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK Chapter 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import gutenberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.corpus.gutenberg.fileids() #Finding text of gutenberg project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "192427"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emma=nltk.corpus.gutenberg.words('austen-emma.txt')\n",
    "len(emma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 25 of 37 matches:\n",
      "er father , was sometimes taken by surprize at his being still able to pity ` \n",
      "hem do the other any good .\" \" You surprize me ! Emma must do Harriet good : a\n",
      "Knightley actually looked red with surprize and displeasure , as he stood up ,\n",
      "r . Elton , and found to his great surprize , that Mr . Elton was actually on \n",
      "d aid .\" Emma saw Mrs . Weston ' s surprize , and felt that it must be great ,\n",
      "father was quite taken up with the surprize of so sudden a journey , and his f\n",
      "y , in all the favouring warmth of surprize and conjecture . She was , moreove\n",
      "he appeared , to have her share of surprize , introduction , and pleasure . Th\n",
      "ir plans ; and it was an agreeable surprize to her , therefore , to perceive t\n",
      "talking aunt had taken me quite by surprize , it must have been the death of m\n",
      "f all the dialogue which ensued of surprize , and inquiry , and congratulation\n",
      " the present . They might chuse to surprize her .\" Mrs . Cole had many to agre\n",
      "the mode of it , the mystery , the surprize , is more like a young woman ' s s\n",
      " to her song took her agreeably by surprize -- a second , slightly but correct\n",
      "\" \" Oh ! no -- there is nothing to surprize one at all .-- A pretty fortune ; \n",
      "t to be considered . Emma ' s only surprize was that Jane Fairfax should accep\n",
      "of your admiration may take you by surprize some day or other .\" Mr . Knightle\n",
      "ation for her will ever take me by surprize .-- I never had a thought of her i\n",
      " expected by the best judges , for surprize -- but there was great joy . Mr . \n",
      " sound of at first , without great surprize . \" So unreasonably early !\" she w\n",
      "d Frank Churchill , with a look of surprize and displeasure .-- \" That is easy\n",
      "; and Emma could imagine with what surprize and mortification she must be retu\n",
      "tled that Jane should go . Quite a surprize to me ! I had not the least idea !\n",
      " . It is impossible to express our surprize . He came to speak to his father o\n",
      "g engaged !\" Emma even jumped with surprize ;-- and , horror - struck , exclai\n"
     ]
    }
   ],
   "source": [
    "nltk.Text(emma).concordance('surprize') #You need to convert it to nltk.Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import gutenberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 24, 26, 'austen-emma.txt')\n",
      "(4, 26, 16, 'austen-persuasion.txt')\n",
      "(4, 28, 22, 'austen-sense.txt')\n",
      "(4, 33, 79, 'bible-kjv.txt')\n",
      "(4, 19, 5, 'blake-poems.txt')\n",
      "(4, 19, 14, 'bryant-stories.txt')\n",
      "(4, 17, 12, 'burgess-busterbrown.txt')\n",
      "(4, 20, 12, 'carroll-alice.txt')\n",
      "(4, 20, 11, 'chesterton-ball.txt')\n",
      "(4, 22, 11, 'chesterton-brown.txt')\n",
      "(4, 18, 10, 'chesterton-thursday.txt')\n",
      "(4, 20, 24, 'edgeworth-parents.txt')\n",
      "(4, 25, 15, 'melville-moby_dick.txt')\n",
      "(4, 52, 10, 'milton-paradise.txt')\n",
      "(4, 11, 8, 'shakespeare-caesar.txt')\n",
      "(4, 12, 7, 'shakespeare-hamlet.txt')\n",
      "(4, 12, 6, 'shakespeare-macbeth.txt')\n",
      "(4, 36, 12, 'whitman-leaves.txt')\n"
     ]
    }
   ],
   "source": [
    "for file in gutenberg.fileids():\n",
    "    num_char=len(gutenberg.raw(file))#No charachters\n",
    "    num_words=len(gutenberg.words(file))#NUmber of words\n",
    "    num_sents=len(gutenberg.sents(file)) #Number of sentences\n",
    "    num_vocab=len(set([w.lower() for w in gutenberg.words(file)])) #unique words\n",
    "    print((int(num_char/num_words), int(num_words/num_sents), int(num_words/num_vocab), file))#Average world length, Average sentence length\n",
    "    #How frequenctly a word is used (lexical Diversity Score)\n",
    "    #The average world length is 4. But it's actually 3. Why? Because it the code counts the spaces too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Time for Assessing Web and Chat Text (Less formal text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import webtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "firefox.txt Cookie Manager: \"Don't allow sites that set removed cookies to se\n",
      "grail.txt SCENE 1: [wind] [clop clop clop] \n",
      "KING ARTHUR: Whoa there!  [clop\n",
      "overheard.txt White guy: So, do you have any plans for this evening?\n",
      "Asian girl\n",
      "pirates.txt PIRATES OF THE CARRIBEAN: DEAD MAN'S CHEST, by Ted Elliott & Terr\n",
      "singles.txt 25 SEXY MALE, seeks attrac older single lady, for discreet encoun\n",
      "wine.txt Lovely delicate, fragrant Rhone wine. Polished leather and strawb\n"
     ]
    }
   ],
   "source": [
    "for field in webtext.fileids():\n",
    "    print(field, webtext.raw(field)[:65])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brown Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['adventure',\n",
       " 'belles_lettres',\n",
       " 'editorial',\n",
       " 'fiction',\n",
       " 'government',\n",
       " 'hobbies',\n",
       " 'humor',\n",
       " 'learned',\n",
       " 'lore',\n",
       " 'mystery',\n",
       " 'news',\n",
       " 'religion',\n",
       " 'reviews',\n",
       " 'romance',\n",
       " 'science_fiction']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "brown.categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', ...]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_words=brown.words(categories='news')\n",
    "news_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100554"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(news_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Thirty-three', 'Scotty', 'did', 'not', 'go', 'back', ...]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w=brown.words(categories='fiction') #Words from a category 'ficiton'\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68488"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Does', 'our', 'society', 'have', 'a', 'runaway', ',', ...]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.words(fileids=['cg22']) #Words from a file cg22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', ...]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_multiple=brown.words(categories=['fiction','news','editorial']) \n",
    "words_multiple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "230646"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words_multiple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  Brown corpus can be used to learn systematic differences between different genres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('can', 94)\n",
      "('could', 87)\n",
      "('may', 93)\n",
      "('might', 38)\n",
      "('must', 53)\n",
      "('will', 389)\n"
     ]
    }
   ],
   "source": [
    "news_text=brown.words(categories='news')\n",
    "fdist= nltk.FreqDist(w.lower() for w in news_text)\n",
    "modals = ['can', 'could', 'may', 'might', 'must', 'will']\n",
    "for m in modals:\n",
    "    print((m, fdist[m]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(fdist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfd = nltk.ConditionalFreqDist((genre, word) for genre in brown.categories() for word in brown.words(categories=genre))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  can could   may might  must  will \n",
      "           news    93    86    66    38    50   389 \n",
      "       religion    82    59    78    12    54    71 \n",
      "        hobbies   268    58   131    22    83   264 \n",
      "science_fiction    16    49     4    12     8    16 \n",
      "        romance    74   193    11    51    45    43 \n",
      "          humor    16    30     8     8     9    13 \n"
     ]
    }
   ],
   "source": [
    "genres = ['news', 'religion', 'hobbies', 'science_fiction', 'romance', 'humor']\n",
    "modal = ['can', 'could', 'may', 'might', 'must', 'will']\n",
    "cfd.tabulate(conditions=genres,  samples=modals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('news', 'Republican-controlled'), ('news', 'Universal-International'), ('news', 'Television-Electronics'), ('news', 'Scotch-Irish-Scandinavian'), ('news', 'Washington-Alexandria'), ('news', 'heavy-electrical-goods'), ('news', 'heavy-electrical-goods'), ('news', 'collective-bargaining'), ('news', 'advertising-conscious'), ('religion', 'culture-Protestantism'), ('religion', \"Preparation-Inquirers'\"), ('religion', 'never-to-be-forgotten'), ('hobbies', 'chest-back-lat-shoulder'), ('hobbies', 'definition-specialization'), ('hobbies', 'definition-specialization'), ('hobbies', 'contraction-extension'), ('hobbies', 'vertical-takeoff-and-landing'), ('hobbies', 'Schubert-Beethoven-Mozart'), ('hobbies', 'mailed-fist-in-velvet-glove'), ('hobbies', \"let's-make-your-house-our-club\"), ('hobbies', 'composer-pianist-conductor'), ('hobbies', 'Commission-controlled'), ('hobbies', 'individual-contributor'), ('hobbies', 'individual-contributor'), ('hobbies', 'engineering-management'), ('hobbies', 'engineering-management'), ('hobbies', 'organization-position'), ('hobbies', 'build-better-for-less'), ('hobbies', 'three-men-and-a-helper'), ('hobbies', 'too-simple-to-be-true'), ('science_fiction', \"dabhumaksanigalu'ahai\"), ('science_fiction', 'delicate-beyond-description'), ('romance', 'princess-in-a-carriage'), ('romance', 'yielding-Mediterranian-woman-'), ('humor', 'non-institutionalized'), ('humor', 'nnuolapertar-it-vuh-karti-birifw-')]\n"
     ]
    }
   ],
   "source": [
    "genres = ['news', 'religion', 'hobbies', 'science_fiction', 'romance', 'humor']\n",
    "genre_words=[(genre, word)for genre in genres for word in brown.words(categories=genre) if len(word)>20]\n",
    "print(genre_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConditionalFreqDist(nltk.probability.FreqDist,\n",
       "                    {'hobbies': FreqDist({'Commission-controlled': 1,\n",
       "                               'Schubert-Beethoven-Mozart': 1,\n",
       "                               'build-better-for-less': 1,\n",
       "                               'chest-back-lat-shoulder': 1,\n",
       "                               'composer-pianist-conductor': 1,\n",
       "                               'contraction-extension': 1,\n",
       "                               'definition-specialization': 2,\n",
       "                               'engineering-management': 2,\n",
       "                               'individual-contributor': 2,\n",
       "                               \"let's-make-your-house-our-club\": 1,\n",
       "                               'mailed-fist-in-velvet-glove': 1,\n",
       "                               'organization-position': 1,\n",
       "                               'three-men-and-a-helper': 1,\n",
       "                               'too-simple-to-be-true': 1,\n",
       "                               'vertical-takeoff-and-landing': 1}),\n",
       "                     'humor': FreqDist({'nnuolapertar-it-vuh-karti-birifw-': 1,\n",
       "                               'non-institutionalized': 1}),\n",
       "                     'news': FreqDist({'Republican-controlled': 1,\n",
       "                               'Scotch-Irish-Scandinavian': 1,\n",
       "                               'Television-Electronics': 1,\n",
       "                               'Universal-International': 1,\n",
       "                               'Washington-Alexandria': 1,\n",
       "                               'advertising-conscious': 1,\n",
       "                               'collective-bargaining': 1,\n",
       "                               'heavy-electrical-goods': 2}),\n",
       "                     'religion': FreqDist({\"Preparation-Inquirers'\": 1,\n",
       "                               'culture-Protestantism': 1,\n",
       "                               'never-to-be-forgotten': 1}),\n",
       "                     'romance': FreqDist({'princess-in-a-carriage': 1,\n",
       "                               'yielding-Mediterranian-woman-': 1}),\n",
       "                     'science_fiction': FreqDist({\"dabhumaksanigalu'ahai\": 1,\n",
       "                               'delicate-beyond-description': 1})})"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfd=nltk.ConditionalFreqDist(genre_words)\n",
    "cfd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  Word counts can distinguish genres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('hobbies', 19), ('hobbies', 23), ('hobbies', 19), ('hobbies', 25), ('hobbies', 25), ('hobbies', 21), ('hobbies', 28), ('hobbies', 25), ('hobbies', 20), ('hobbies', 19), ('hobbies', 17), ('hobbies', 18), ('hobbies', 17), ('hobbies', 18), ('hobbies', 18), ('hobbies', 17), ('hobbies', 19), ('hobbies', 18), ('hobbies', 19), ('hobbies', 19), ('hobbies', 19), ('hobbies', 18), ('hobbies', 20), ('hobbies', 17), ('hobbies', 27), ('hobbies', 30), ('hobbies', 19), ('hobbies', 26), ('hobbies', 17), ('hobbies', 17), ('hobbies', 19), ('hobbies', 19), ('hobbies', 18), ('hobbies', 18), ('hobbies', 17), ('hobbies', 17), ('hobbies', 18), ('hobbies', 18), ('hobbies', 18), ('hobbies', 18), ('hobbies', 19), ('hobbies', 19), ('hobbies', 19), ('hobbies', 17), ('hobbies', 18), ('hobbies', 18), ('hobbies', 20), ('hobbies', 17), ('hobbies', 21), ('hobbies', 18), ('hobbies', 22), ('hobbies', 22), ('hobbies', 22), ('hobbies', 22), ('hobbies', 21), ('hobbies', 19), ('hobbies', 18), ('hobbies', 18), ('hobbies', 17), ('hobbies', 17), ('hobbies', 17), ('hobbies', 17), ('hobbies', 17), ('hobbies', 17), ('hobbies', 17), ('hobbies', 17), ('hobbies', 17), ('hobbies', 21), ('hobbies', 18), ('hobbies', 22), ('hobbies', 19), ('hobbies', 21), ('science_fiction', 19), ('science_fiction', 18), ('science_fiction', 18), ('science_fiction', 21), ('science_fiction', 17), ('science_fiction', 19), ('science_fiction', 27), ('romance', 22), ('romance', 19), ('romance', 17), ('romance', 19), ('romance', 17), ('romance', 17), ('romance', 18), ('romance', 17), ('romance', 18), ('romance', 19), ('romance', 17), ('romance', 29), ('romance', 20), ('romance', 19)]\n"
     ]
    }
   ],
   "source": [
    "genres=['hobbies','science_fiction', 'romance']\n",
    "word_length=[(genre, len(word))for genre in genres for word in brown.words(categories=genre) if len(word)>16]\n",
    "print(word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                17 18 19 20 \n",
      "        hobbies 19 17 15  3 \n",
      "science_fiction  1  2  2  0 \n",
      "        romance  5  2  4  1 \n"
     ]
    }
   ],
   "source": [
    "cfd=nltk.ConditionalFreqDist(word_length)\n",
    "cfd.tabulate(conditions=genres, samples=[i for i in range(17, 21)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 3 of 3 matches:\n",
      "ions to `` come over next summer and swim in our new pool '' were both unexpec\n",
      "eing our lingo for those who come to swim and sink into our bar while protesti\n",
      "e when neighborhood teen-agers might swim at definite hours . This has saved u\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "news_text=brown.words(categories='hobbies')\n",
    "print(nltk.Text(news_text).concordance('swim'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New York; chemical name; United States; Drug's chemical; per head;\n",
      "years ago; interior design; Santa Cruz; milligrams per; nuclear\n",
      "weapons; locking bars; St. Sophia; combustion chamber; bacterial\n",
      "diarrhea; per day; locking bar; Junior Showmanship; grams per; air\n",
      "conditioning; per pound\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "news_text=brown.words(categories='hobbies')\n",
    "print(nltk.Text(news_text).collocations())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Reutuers Corpus (news documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Categorised into 90 topics and grouped into two sets 'Training' and 'Testing'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import reuters\n",
    "reuters.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reuters.categories() #These categories overlap with each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reuters.categories('training/9865')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reuters.categories(['training/9865', 'training/9866', 'training/9880'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reuters.fileids('barley')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reuters.fileids(['barley', 'corn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reuters.words(categories='barley')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reuters.words(categories=['barley','corn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reuters.words(fileids=['training/9865', 'training/9880'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reuters.words(['training/9865', 'training/9880'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Inaugural Address Corpus (of Presidents?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import inaugural\n",
    "inaugural.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[files[:4] for files in inaugural.fileids()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfd=nltk.ConditionalFreqDist((target, file[:4]) for file in inaugural.fileids() for w in inaugural.words(file)\n",
    "                             for target in ['america','citizen'] if w.lower().startswith(target)) \n",
    "cfd.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditional Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "######  FreqDist returns a list of frequencies for each word. Conditional Frequency returns a list of Frequency Distribution and each frequency distribution has a condition associated with it. For example if the condition is genre meaning for each genre get its frequency distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g=[(genre, word) for genre in ['news', 'romance'] for word in brown.words(categories=genre)]\n",
    "print(g[:4])\n",
    "print(g[-4:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfd= nltk.ConditionalFreqDist((genre, word) for genre in ['news', 'romance'] for word in brown.words(categories=genre))\n",
    "cfd.conditions() #It has two conditions news and romance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfd= nltk.ConditionalFreqDist((genre, word) for genre in ['news', 'romance','adventure'] for word in brown.words(categories=genre))\n",
    "cfd.conditions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Plotting and Tabulating Distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  In the plot() and tabulate() we can exploit the conditions parameter and specify which condtions to display. By default we get all the conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  0    1    2    3    4    5    6    7    8    9 \n",
      "       English    0  185  525  883  997 1166 1283 1440 1558 1638 \n",
      "German_Deutsch    0  171  263  614  717  894 1013 1110 1213 1275 \n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import udhr\n",
    "languages = ['English', 'Chickasaw', 'German_Deutsch', 'Greenlandic_Inuktikut', 'Hungarian_Magyar', 'Ibibio_Efik']\n",
    "cfd = nltk.ConditionalFreqDist((lang, len(word)) for lang in languages for word in udhr.words(lang+'-Latin1'))\n",
    "cfd.tabulate(conditions=['English', 'German_Deutsch'], samples=range(10), cumulative=True) #samples takes a list of keys to use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### We interpret above example first row last cell that there are 1638 words with length 9 or lower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConditionalFreqDist(nltk.probability.FreqDist,\n",
       "                    {'Chickasaw': FreqDist({1: 411,\n",
       "                               2: 99,\n",
       "                               3: 41,\n",
       "                               4: 68,\n",
       "                               5: 91,\n",
       "                               6: 89,\n",
       "                               7: 77,\n",
       "                               8: 70,\n",
       "                               9: 49,\n",
       "                               10: 33,\n",
       "                               11: 16,\n",
       "                               12: 28,\n",
       "                               13: 45,\n",
       "                               14: 10,\n",
       "                               15: 6,\n",
       "                               16: 4,\n",
       "                               17: 5,\n",
       "                               18: 3,\n",
       "                               19: 2,\n",
       "                               20: 1,\n",
       "                               21: 1,\n",
       "                               23: 1}),\n",
       "                     'English': FreqDist({1: 185,\n",
       "                               2: 340,\n",
       "                               3: 358,\n",
       "                               4: 114,\n",
       "                               5: 169,\n",
       "                               6: 117,\n",
       "                               7: 157,\n",
       "                               8: 118,\n",
       "                               9: 80,\n",
       "                               10: 63,\n",
       "                               11: 50,\n",
       "                               12: 12,\n",
       "                               13: 11,\n",
       "                               14: 6,\n",
       "                               15: 1}),\n",
       "                     'German_Deutsch': FreqDist({1: 171,\n",
       "                               2: 92,\n",
       "                               3: 351,\n",
       "                               4: 103,\n",
       "                               5: 177,\n",
       "                               6: 119,\n",
       "                               7: 97,\n",
       "                               8: 103,\n",
       "                               9: 62,\n",
       "                               10: 58,\n",
       "                               11: 53,\n",
       "                               12: 32,\n",
       "                               13: 27,\n",
       "                               14: 29,\n",
       "                               15: 15,\n",
       "                               16: 14,\n",
       "                               17: 3,\n",
       "                               18: 7,\n",
       "                               19: 5,\n",
       "                               20: 2,\n",
       "                               21: 1}),\n",
       "                     'Greenlandic_Inuktikut': FreqDist({1: 139,\n",
       "                               2: 11,\n",
       "                               3: 1,\n",
       "                               4: 3,\n",
       "                               5: 21,\n",
       "                               6: 7,\n",
       "                               7: 59,\n",
       "                               8: 18,\n",
       "                               9: 24,\n",
       "                               10: 23,\n",
       "                               11: 38,\n",
       "                               12: 61,\n",
       "                               13: 46,\n",
       "                               14: 27,\n",
       "                               15: 30,\n",
       "                               16: 38,\n",
       "                               17: 25,\n",
       "                               18: 26,\n",
       "                               19: 16,\n",
       "                               20: 23,\n",
       "                               21: 15,\n",
       "                               22: 13,\n",
       "                               23: 23,\n",
       "                               24: 16,\n",
       "                               25: 12,\n",
       "                               26: 5,\n",
       "                               27: 7,\n",
       "                               28: 10,\n",
       "                               29: 4,\n",
       "                               30: 5,\n",
       "                               31: 2,\n",
       "                               32: 4,\n",
       "                               33: 5,\n",
       "                               34: 1,\n",
       "                               35: 1,\n",
       "                               36: 1,\n",
       "                               37: 1}),\n",
       "                     'Hungarian_Magyar': FreqDist({1: 302,\n",
       "                               2: 129,\n",
       "                               3: 72,\n",
       "                               4: 152,\n",
       "                               5: 112,\n",
       "                               6: 114,\n",
       "                               7: 91,\n",
       "                               8: 109,\n",
       "                               9: 90,\n",
       "                               10: 105,\n",
       "                               11: 71,\n",
       "                               12: 48,\n",
       "                               13: 38,\n",
       "                               14: 17,\n",
       "                               15: 13,\n",
       "                               16: 8,\n",
       "                               17: 3,\n",
       "                               18: 7,\n",
       "                               19: 2,\n",
       "                               20: 1,\n",
       "                               22: 1}),\n",
       "                     'Ibibio_Efik': FreqDist({1: 228,\n",
       "                               2: 212,\n",
       "                               3: 475,\n",
       "                               4: 503,\n",
       "                               5: 287,\n",
       "                               6: 162,\n",
       "                               7: 107,\n",
       "                               8: 75,\n",
       "                               9: 25,\n",
       "                               10: 4})})"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['adventure',\n",
       " 'belles_lettres',\n",
       " 'editorial',\n",
       " 'fiction',\n",
       " 'government',\n",
       " 'hobbies',\n",
       " 'humor',\n",
       " 'learned',\n",
       " 'lore',\n",
       " 'mystery',\n",
       " 'news',\n",
       " 'religion',\n",
       " 'reviews',\n",
       " 'romance',\n",
       " 'science_fiction']"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "days=['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "brown.categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Friday    Monday  Saturday    Sunday  Thursday   Tuesday Wednesday \n",
      "   news        41        54        33        51        20        43        22 \n",
      "romance         3         2         4         5         1         3         3 \n"
     ]
    }
   ],
   "source": [
    "cfd=nltk.ConditionalFreqDist((genre, word) for genre in ['news', 'romance'] for word in brown.words(categories=genre) if word in days)\n",
    "cfd.tabulate(sample=days, cumulative=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating random text with Bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### bigrams() takes a list of words and returns a list of consecutive word pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_model(cfdist, word, num=15):\n",
    "    for i in range(num):\n",
    "        print(word)\n",
    "        word=cfdist[word].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=nltk.corpus.genesis.words('english-kjv.txt')\n",
    "bigrams=nltk.bigrams(text)\n",
    "cfd=nltk.ConditionalFreqDist(bigrams)\n",
    "cfd['living']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_model(cfd, 'living')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfd['creature']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wordlist Corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unusual_words(text):\n",
    "    text_vocab=set(w.lower() for w in text if w.isalpha())\n",
    "    english_vocab=set(w.lower() for w in nltk.corpus.words.words())\n",
    "    unusual_words=text_vocab.difference(english_vocab)\n",
    "    return sorted(unusual_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unusual_words(gutenberg.words('austen-sense.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  Stopwords are words like or, the, a, an wtc which don't have a lot of lexical content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "sorted(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def content_fraction(text):\n",
    "    \"\"\"returns fraction of words not in the stopwords\"\"\"\n",
    "    stopwords=nltk.corpus.stopwords.words('english')\n",
    "    content=[w for w in text if w.lower() not in stopwords]\n",
    "    return len(content)/len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_fraction(reuters.words())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Above we have used a lexical resource to filter the contents of text resource. 2 corpusses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "puzzle_letters= nltk.FreqDist('egivrconl')\n",
    "obligatory='r'\n",
    "wordlist=nltk.corpus.words.words()\n",
    "[w for w in wordlist if len(w)>= 6 and obligatory in w and nltk.FreqDist(w) <= puzzle_letters]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "puzzle_letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import names\n",
    "cfd=nltk.ConditionalFreqDist((file,name[-1]) for file in names.fileids() for name in names.words(file))\n",
    "cfd.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  Female names end with a."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pronouncing Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### CMU Pronouncing Dictionary for English is designed for speech synthesis. Richer lexical resource because with a word it also has addtional properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "133737"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entries=nltk.corpus.cmudict.entries()\n",
    "len(entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('aloisio', ['AA0', 'L', 'OY1', 'S', 'IY0', 'OW0']),\n",
       " ('aloka', ['AH0', 'L', 'OW1', 'K', 'AH0']),\n",
       " ('alomar', ['AE1', 'L', 'AH0', 'M', 'AA2', 'R']),\n",
       " ('alon', ['AH0', 'L', 'AA1', 'N']),\n",
       " ('alone', ['AH0', 'L', 'OW1', 'N']),\n",
       " ('along', ['AH0', 'L', 'AO1', 'NG']),\n",
       " ('alonge', ['AE1', 'L', 'AH0', 'N', 'JH']),\n",
       " ('alongi', ['AA0', 'L', 'OW1', 'NG', 'G', 'IY0'])]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entries[3043:3051] #The second list elements are called phones which are distinct labels for each contrastive sound of the word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('pait', 'EY1')\n",
      "('pat', 'AE1')\n",
      "('pate', 'EY1')\n",
      "('patt', 'AE1')\n",
      "('peart', 'ER1')\n",
      "('peat', 'IY1')\n",
      "('peet', 'IY1')\n",
      "('peete', 'IY1')\n",
      "('pert', 'ER1')\n",
      "('pet', 'EH1')\n",
      "('pete', 'IY1')\n",
      "('pett', 'EH1')\n",
      "('piet', 'IY1')\n",
      "('piette', 'IY1')\n",
      "('pit', 'IH1')\n",
      "('pitt', 'IH1')\n",
      "('pot', 'AA1')\n",
      "('pote', 'OW1')\n",
      "('pott', 'AA1')\n",
      "('pout', 'AW1')\n",
      "('puett', 'UW1')\n",
      "('purt', 'ER1')\n",
      "('put', 'UH1')\n",
      "('putt', 'AH1')\n"
     ]
    }
   ],
   "source": [
    "for word, pron in entries:\n",
    "    if len(pron)==3:\n",
    "        ph1, ph2, ph3 = pron\n",
    "        if ph1=='P' and ph3=='T':\n",
    "            print((word, ph2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  The next program finds words whose pronunciation ends with syllable sounding like nicks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"atlantic's\",\n",
       " 'audiotronics',\n",
       " 'avionics',\n",
       " 'beatniks',\n",
       " 'calisthenics',\n",
       " 'centronics',\n",
       " 'chamonix',\n",
       " 'chetniks',\n",
       " \"clinic's\",\n",
       " 'clinics',\n",
       " 'conics',\n",
       " 'conics',\n",
       " 'cryogenics',\n",
       " 'cynics',\n",
       " 'diasonics',\n",
       " \"dominic's\",\n",
       " 'ebonics',\n",
       " 'electronics',\n",
       " \"electronics'\",\n",
       " \"endotronics'\",\n",
       " 'endotronics',\n",
       " 'enix',\n",
       " 'environics',\n",
       " 'ethnics',\n",
       " 'eugenics',\n",
       " 'fibronics',\n",
       " 'flextronics',\n",
       " 'harmonics',\n",
       " 'hispanics',\n",
       " 'histrionics',\n",
       " 'identics',\n",
       " 'ionics',\n",
       " 'kibbutzniks',\n",
       " 'lasersonics',\n",
       " 'lumonics',\n",
       " 'mannix',\n",
       " 'mechanics',\n",
       " \"mechanics'\",\n",
       " 'microelectronics',\n",
       " 'minix',\n",
       " 'minnix',\n",
       " 'mnemonics',\n",
       " 'mnemonics',\n",
       " 'molonicks',\n",
       " 'mullenix',\n",
       " 'mullenix',\n",
       " 'mullinix',\n",
       " 'mulnix',\n",
       " \"munich's\",\n",
       " 'nucleonics',\n",
       " 'onyx',\n",
       " 'organics',\n",
       " \"panic's\",\n",
       " 'panics',\n",
       " 'penix',\n",
       " 'pennix',\n",
       " 'personics',\n",
       " 'phenix',\n",
       " \"philharmonic's\",\n",
       " 'phoenix',\n",
       " 'phonics',\n",
       " 'photronics',\n",
       " 'pinnix',\n",
       " 'plantronics',\n",
       " 'pyrotechnics',\n",
       " 'refuseniks',\n",
       " \"resnick's\",\n",
       " 'respironics',\n",
       " 'sconnix',\n",
       " 'siliconix',\n",
       " 'skolniks',\n",
       " 'sonics',\n",
       " 'sputniks',\n",
       " 'technics',\n",
       " 'tectonics',\n",
       " 'tektronix',\n",
       " 'telectronics',\n",
       " 'telephonics',\n",
       " 'tonics',\n",
       " 'unix',\n",
       " \"vinick's\",\n",
       " \"vinnick's\",\n",
       " 'vitronics']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syllable=['N', 'IH0','K','S']\n",
    "[w for w, pron in entries if pron[-4:] == syllable]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mismatches between pronunciation and writing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['autumn', 'column', 'condemn', 'damn', 'goddamn', 'hymn', 'solemn']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[w for w, pros in entries if pros[-1].lower()=='m' and w[-1]=='n'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gn', 'kn', 'mn', 'pn']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(set([w[:2] for w, pros in entries if pros[0].lower() == 'n' and w[0] != 'n']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  The phones contain digits to contain primary stress(1), secondary stress(2), and not stress(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stress(pros):\n",
    "    return [w for phone in pros for w in phone if w.isdigit()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abbreviated',\n",
       " 'abbreviated',\n",
       " 'abbreviating',\n",
       " 'accelerated',\n",
       " 'accelerating',\n",
       " 'accelerator',\n",
       " 'accelerators',\n",
       " 'accentuated',\n",
       " 'accentuating',\n",
       " 'accommodated',\n",
       " 'accommodating',\n",
       " 'accommodative',\n",
       " 'accumulated',\n",
       " 'accumulating',\n",
       " 'accumulative',\n",
       " 'accumulator',\n",
       " 'accumulators',\n",
       " 'accusatory',\n",
       " 'adenovirus',\n",
       " 'adjudicated',\n",
       " 'adjudicating',\n",
       " 'administrating',\n",
       " 'administrative',\n",
       " 'administrator',\n",
       " \"administrators'\",\n",
       " \"administrator's\",\n",
       " 'administrators',\n",
       " 'adulterated',\n",
       " 'adventurism',\n",
       " 'adventurism',\n",
       " 'affiliated',\n",
       " 'affiliated',\n",
       " \"affiliated's\",\n",
       " 'affiliating',\n",
       " 'alleviated',\n",
       " 'alleviated',\n",
       " 'alleviating',\n",
       " 'alliteration',\n",
       " 'alliterative',\n",
       " 'amalgamated',\n",
       " \"amalgamated's\",\n",
       " 'amalgamating',\n",
       " 'ameliorated',\n",
       " 'ameridata',\n",
       " 'amoxicillin',\n",
       " 'anachronism',\n",
       " 'anachronisms',\n",
       " 'annihilated',\n",
       " 'annihilating',\n",
       " 'antagonism',\n",
       " 'antagonisms',\n",
       " 'antagonizing',\n",
       " 'anticipated',\n",
       " 'anticipated',\n",
       " 'anticipating',\n",
       " 'apologizes',\n",
       " 'apologizing',\n",
       " 'apothecary',\n",
       " 'appreciated',\n",
       " 'appreciating',\n",
       " 'appreciative',\n",
       " 'appropriated',\n",
       " 'appropriating',\n",
       " 'appropriator',\n",
       " 'appropriators',\n",
       " 'approximated',\n",
       " 'approximating',\n",
       " 'articulated',\n",
       " 'articulating',\n",
       " 'asphyxiated',\n",
       " 'asphyxiating',\n",
       " 'assassinated',\n",
       " 'assassinating',\n",
       " 'assemblywoman',\n",
       " 'assimilated',\n",
       " 'assimilating',\n",
       " 'associated',\n",
       " 'associated',\n",
       " 'associating',\n",
       " 'astigmatism',\n",
       " 'attenuated',\n",
       " 'authenticated',\n",
       " 'authenticating',\n",
       " 'authoritative',\n",
       " 'bicentenary',\n",
       " 'bilingualism',\n",
       " 'biosciences',\n",
       " 'buchananism',\n",
       " 'cadiddlehopper',\n",
       " 'capitulated',\n",
       " 'catholicism',\n",
       " 'celebratory',\n",
       " 'coagulating',\n",
       " 'cogenerator',\n",
       " 'cogenerators',\n",
       " 'collaborated',\n",
       " 'collaborated',\n",
       " 'collaborating',\n",
       " 'collaborative',\n",
       " 'collaborator',\n",
       " 'collaborators',\n",
       " 'collectivism',\n",
       " 'commemorated',\n",
       " 'commemorating',\n",
       " 'commemorative',\n",
       " 'commercialism',\n",
       " 'commercializing',\n",
       " 'communicated',\n",
       " 'communicating',\n",
       " 'communicator',\n",
       " 'compensatory',\n",
       " 'computerizing',\n",
       " 'computervision',\n",
       " 'concatenated',\n",
       " 'concatenating',\n",
       " 'concessionary',\n",
       " 'conciliator',\n",
       " \"conciliator's\",\n",
       " 'conciliatory',\n",
       " 'confectionaries',\n",
       " 'confectionary',\n",
       " 'confectionery',\n",
       " 'confirmatory',\n",
       " 'confiscatory',\n",
       " 'confucianism',\n",
       " 'congratulated',\n",
       " 'congratulating',\n",
       " 'conservatism',\n",
       " 'conservatories',\n",
       " 'consolidated',\n",
       " \"consolidated's\",\n",
       " 'consolidating',\n",
       " 'consolidator',\n",
       " 'consolidators',\n",
       " 'constabulary',\n",
       " 'construcciones',\n",
       " 'consumerism',\n",
       " 'contaminated',\n",
       " 'contaminated',\n",
       " 'contaminating',\n",
       " 'contemporaries',\n",
       " 'contemporary',\n",
       " 'contributory',\n",
       " 'cooperated',\n",
       " 'cooperating',\n",
       " 'cooperative',\n",
       " 'coordinating',\n",
       " 'coordinator',\n",
       " 'coordinators',\n",
       " 'corroborated',\n",
       " 'corroborating',\n",
       " 'creationism',\n",
       " 'debilitated',\n",
       " 'debilitating',\n",
       " 'decaffeinated',\n",
       " 'decaffeinating',\n",
       " 'decapitated',\n",
       " 'decelerated',\n",
       " 'decelerating',\n",
       " 'decentralizing',\n",
       " 'decisionmaker',\n",
       " 'decisionmaking',\n",
       " 'declaratory',\n",
       " 'deemphasizing',\n",
       " 'defamatory',\n",
       " 'defibrillator',\n",
       " 'defibrillators',\n",
       " 'deflationary',\n",
       " 'degenerated',\n",
       " 'degenerating',\n",
       " 'dehumanizing',\n",
       " 'deliberated',\n",
       " 'deliberating',\n",
       " 'deliberative',\n",
       " 'delineated',\n",
       " 'delineating',\n",
       " 'demobilizes',\n",
       " 'demobilizing',\n",
       " 'democratizes',\n",
       " 'democratizing',\n",
       " 'demoralizing',\n",
       " 'denominated',\n",
       " 'denominator',\n",
       " 'depilatory',\n",
       " 'depositary',\n",
       " 'depositary',\n",
       " 'depository',\n",
       " 'depreciated',\n",
       " 'depreciating',\n",
       " 'depressurizes',\n",
       " 'depressurizing',\n",
       " 'deregulating',\n",
       " 'derogatory',\n",
       " 'desegregated',\n",
       " 'desensitizing',\n",
       " 'destabilizing',\n",
       " 'determinism',\n",
       " 'devaluated',\n",
       " 'diastrophism',\n",
       " 'dilapidated',\n",
       " 'discretionary',\n",
       " 'discriminated',\n",
       " 'discriminated',\n",
       " 'discriminating',\n",
       " 'disintegrated',\n",
       " 'disintegrating',\n",
       " 'disoriented',\n",
       " 'disorienting',\n",
       " 'disqualifying',\n",
       " 'disseminated',\n",
       " 'disseminating',\n",
       " 'diversifying',\n",
       " 'diversifying',\n",
       " 'diversionary',\n",
       " 'diversionary',\n",
       " 'domesticated',\n",
       " 'domesticating',\n",
       " 'economizes',\n",
       " 'economizes',\n",
       " 'economizing',\n",
       " 'economizing',\n",
       " 'elaborating',\n",
       " 'electrifying',\n",
       " 'electrocuted',\n",
       " 'electroplating',\n",
       " 'eliminated',\n",
       " 'eliminated',\n",
       " 'eliminating',\n",
       " 'elucidated',\n",
       " 'elucidative',\n",
       " 'emaciated',\n",
       " 'emaciating',\n",
       " 'emancipated',\n",
       " 'emancipating',\n",
       " 'emasculated',\n",
       " 'empiricism',\n",
       " 'emulsifier',\n",
       " 'encapsulated',\n",
       " 'encapsulating',\n",
       " 'enthusiasm',\n",
       " 'enthusiasms',\n",
       " 'enumerated',\n",
       " 'enunciated',\n",
       " 'enunciating',\n",
       " 'epistolary',\n",
       " 'epitomizes',\n",
       " 'equivocating',\n",
       " 'eradicated',\n",
       " 'eradicating',\n",
       " 'eroticism',\n",
       " 'evacuated',\n",
       " 'evacuated',\n",
       " 'evacuating',\n",
       " 'evacuating',\n",
       " 'evaluated',\n",
       " 'evaluated',\n",
       " 'evaluating',\n",
       " 'evaluating',\n",
       " 'evangelism',\n",
       " 'evangelism',\n",
       " 'evaporated',\n",
       " 'evaporated',\n",
       " 'evaporated',\n",
       " 'evaporated',\n",
       " 'evaporating',\n",
       " 'evaporating',\n",
       " 'evaporator',\n",
       " 'evaporator',\n",
       " 'eviscerated',\n",
       " 'exacerbated',\n",
       " 'exacerbated',\n",
       " 'exacerbating',\n",
       " 'exaggerated',\n",
       " 'exaggerated',\n",
       " 'exaggerating',\n",
       " 'exasperated',\n",
       " 'exasperating',\n",
       " 'exclusionary',\n",
       " 'excoriated',\n",
       " 'excoriating',\n",
       " 'excoriation',\n",
       " 'excruciating',\n",
       " 'exemplifying',\n",
       " 'exhilarated',\n",
       " 'exhilarating',\n",
       " 'exonerated',\n",
       " 'exonerating',\n",
       " 'expansionary',\n",
       " 'expansionary',\n",
       " 'expansionism',\n",
       " 'expansionism',\n",
       " 'experimenter',\n",
       " 'experimenters',\n",
       " 'experimenting',\n",
       " 'expiratory',\n",
       " 'explanatory',\n",
       " 'exploratory',\n",
       " 'exploravision',\n",
       " 'expressionism',\n",
       " 'expropriated',\n",
       " 'extenuating',\n",
       " 'exterminated',\n",
       " 'exterminating',\n",
       " 'exterminator',\n",
       " 'exterminators',\n",
       " 'extraordinary',\n",
       " 'extrapolated',\n",
       " 'extrapolating',\n",
       " 'facilitated',\n",
       " 'facilitating',\n",
       " 'facilitator',\n",
       " \"facilitator's\",\n",
       " 'facilitators',\n",
       " 'fanaticism',\n",
       " 'fiduciares',\n",
       " 'fiduciaries',\n",
       " 'fiduciary',\n",
       " 'ganatieuganauf',\n",
       " 'geotropism',\n",
       " 'hereditary',\n",
       " 'humidifier',\n",
       " 'humidifiers',\n",
       " 'humiliated',\n",
       " 'humiliating',\n",
       " 'hydrogenated',\n",
       " 'identifier',\n",
       " 'identifier',\n",
       " 'identifying',\n",
       " 'identifying',\n",
       " 'illuminated',\n",
       " 'illuminating',\n",
       " 'illuminator',\n",
       " 'illusionary',\n",
       " 'illusionism',\n",
       " 'imaginary',\n",
       " 'immeasurable',\n",
       " 'immeasurably',\n",
       " 'immobilizing',\n",
       " 'impersonated',\n",
       " 'impersonating',\n",
       " 'impersonators',\n",
       " 'impressionism',\n",
       " 'inaccuracies',\n",
       " 'inactivated',\n",
       " 'inaugurated',\n",
       " 'inaugurated',\n",
       " 'inaugurating',\n",
       " 'incantatory',\n",
       " 'incarcerated',\n",
       " 'incarcerating',\n",
       " 'incinerated',\n",
       " 'incinerating',\n",
       " 'incineration',\n",
       " 'incinerator',\n",
       " 'incinerators',\n",
       " 'inconsistencies',\n",
       " 'incorporated',\n",
       " 'incorporated',\n",
       " \"incorporated's\",\n",
       " 'incorporating',\n",
       " 'incriminating',\n",
       " 'indisputably',\n",
       " 'indoctrinated',\n",
       " 'inebriated',\n",
       " 'inebriating',\n",
       " 'infatuated',\n",
       " 'infatuating',\n",
       " 'inflammatory',\n",
       " 'inflationary',\n",
       " 'infuriated',\n",
       " 'infuriated',\n",
       " 'infuriating',\n",
       " 'ingratiating',\n",
       " 'inhibitory',\n",
       " 'initiated',\n",
       " 'initiated',\n",
       " 'initiating',\n",
       " 'innoculated',\n",
       " 'innoculating',\n",
       " 'inoculated',\n",
       " 'instantiated',\n",
       " 'instantiating',\n",
       " 'intensifying',\n",
       " 'interrogated',\n",
       " 'interrogating',\n",
       " 'intimidated',\n",
       " 'intimidating',\n",
       " 'intoxicated',\n",
       " 'intoxicated',\n",
       " 'intoxicating',\n",
       " 'invalidated',\n",
       " 'invalidated',\n",
       " 'invalidating',\n",
       " 'investigated',\n",
       " 'investigated',\n",
       " 'investigating',\n",
       " 'investigative',\n",
       " 'investigator',\n",
       " \"investigator's\",\n",
       " 'investigators',\n",
       " \"investigators'\",\n",
       " 'invigorated',\n",
       " 'invigorating',\n",
       " 'involuntary',\n",
       " 'irradiated',\n",
       " 'itineraries',\n",
       " 'itinerary',\n",
       " 'judiciary',\n",
       " 'legitimizes',\n",
       " 'legitimizing',\n",
       " 'manipulated',\n",
       " 'manipulating',\n",
       " 'manipulative',\n",
       " 'manipulator',\n",
       " 'manipulators',\n",
       " 'mercantilism',\n",
       " 'metabolism',\n",
       " 'metabolisms',\n",
       " 'misallocated',\n",
       " 'misallocating',\n",
       " 'miscalculated',\n",
       " 'misrecognizes',\n",
       " 'misrecognizing',\n",
       " 'monasticism',\n",
       " 'monopolizes',\n",
       " 'monopolizing',\n",
       " 'necessitated',\n",
       " 'necessitating',\n",
       " 'negotiated',\n",
       " 'negotiated',\n",
       " 'negotiating',\n",
       " 'negotiator',\n",
       " 'negotiator',\n",
       " \"negotiators'\",\n",
       " \"negotiator's\",\n",
       " 'negotiators',\n",
       " 'nonmilitary',\n",
       " 'nonregulated',\n",
       " 'obituaries',\n",
       " 'obituary',\n",
       " 'obligatory',\n",
       " 'obliterated',\n",
       " 'obliterating',\n",
       " 'observatories',\n",
       " 'observatory',\n",
       " \"observatory's\",\n",
       " 'obstructionism',\n",
       " 'officiated',\n",
       " 'officiating',\n",
       " 'opinionated',\n",
       " 'originated',\n",
       " 'originated',\n",
       " 'originating',\n",
       " 'originator',\n",
       " 'originators',\n",
       " 'participated',\n",
       " 'participated',\n",
       " 'participating',\n",
       " 'paternalism',\n",
       " 'pecuniary',\n",
       " 'perfectionism',\n",
       " 'perpetuated',\n",
       " 'perpetuating',\n",
       " 'personifying',\n",
       " 'pituitary',\n",
       " 'pituitary',\n",
       " 'politicizing',\n",
       " 'pontificated',\n",
       " 'pontificater',\n",
       " 'pontificaters',\n",
       " 'pontificating',\n",
       " 'precipitated',\n",
       " 'precipitating',\n",
       " 'predominated',\n",
       " 'predominating',\n",
       " 'prefabricated',\n",
       " 'prekindergarten',\n",
       " 'preliminaries',\n",
       " 'preliminaries',\n",
       " 'preliminary',\n",
       " 'preliminary',\n",
       " 'premeditated',\n",
       " 'preparatory',\n",
       " 'prioritizes',\n",
       " 'prioritizing',\n",
       " 'probationary',\n",
       " 'procrastinated',\n",
       " 'procrastinating',\n",
       " 'procrastinator',\n",
       " 'procrastinators',\n",
       " 'prohibitory',\n",
       " 'proliferated',\n",
       " 'proliferating',\n",
       " 'proprietaries',\n",
       " 'proprietary',\n",
       " 'protectionism',\n",
       " 'protectionism',\n",
       " 'provincialism',\n",
       " 'reactionaries',\n",
       " 'reactionary',\n",
       " 'reallocating',\n",
       " 'reanalyses',\n",
       " 'reanalysing',\n",
       " 'reauthorizing',\n",
       " 'recalculated',\n",
       " 'recalculating',\n",
       " 'recessionary',\n",
       " 'recidivism',\n",
       " 'reciprocated',\n",
       " 'reciprocating',\n",
       " 'reclassifying',\n",
       " 'reconstituted',\n",
       " 'reconstituting',\n",
       " 'recuperated',\n",
       " 'recuperater',\n",
       " 'recuperating',\n",
       " 'recuperating',\n",
       " 'redecorated',\n",
       " 'redecorating',\n",
       " 'reformatories',\n",
       " 'reformatory',\n",
       " 'reformulated',\n",
       " 'refrigerated',\n",
       " 'refrigerator',\n",
       " 'refrigerator',\n",
       " 'refrigerators',\n",
       " 'regenerated',\n",
       " 'regenerating',\n",
       " 'reinstituting',\n",
       " 'reintegrated',\n",
       " 'reintegration',\n",
       " 'reiterated',\n",
       " 'reiterating',\n",
       " 'rejuvenated',\n",
       " 'rejuvenating',\n",
       " 'renominated',\n",
       " 'reorganizes',\n",
       " 'reorganizing',\n",
       " 'repatriated',\n",
       " 'repatriating',\n",
       " 'repositories',\n",
       " 'repository',\n",
       " 'repudiated',\n",
       " 'repudiating',\n",
       " 'resuscitated',\n",
       " 'resuscitating',\n",
       " 'retaliated',\n",
       " 'retaliated',\n",
       " 'retaliating',\n",
       " 'retaliatory',\n",
       " 'revelatory',\n",
       " 'reverberated',\n",
       " 'reverberated',\n",
       " 'reverberated',\n",
       " 'reverberating',\n",
       " 'reverberating',\n",
       " 'revisionism',\n",
       " 'revitalizing',\n",
       " 'romanticism',\n",
       " 'romanticizing',\n",
       " 'securitizing',\n",
       " 'solidifying',\n",
       " 'sophisticated',\n",
       " 'sophisticated',\n",
       " 'subordinated',\n",
       " 'subordinating',\n",
       " 'subsidiaries',\n",
       " \"subsidiaries'\",\n",
       " 'subsidiary',\n",
       " \"subsidiary's\",\n",
       " 'substantiated',\n",
       " 'substantiated',\n",
       " 'surrealism',\n",
       " \"surrealism's\",\n",
       " 'surrealisms',\n",
       " 'unallocated',\n",
       " 'uncompensated',\n",
       " 'uncomplicated',\n",
       " 'uneducated',\n",
       " 'unmitigated',\n",
       " 'unnecessary',\n",
       " 'unprecedented',\n",
       " 'unregulated',\n",
       " 'unsanitary',\n",
       " 'unsatisfying',\n",
       " 'unsaturated',\n",
       " 'velociraptor',\n",
       " 'vocabulary',\n",
       " 'voluntarism']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[w for w, pros in entries if stress(pros)==['0','1','0','2', '0']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P-P paap paape pap pape papp paup peep pep pip pipe pipp poop pop pope pop..\n",
      "P-R paar pair par pare parr pear peer pier poor poore por pore porr pour..\n",
      "P-K pac pack paek paik pak pake paque peak peake pech peck peek perc perk ..\n",
      "P-S pace pass pasts peace pearse pease perce pers perse pesce piece piss p..\n",
      "P-L pahl pail paille pal pale pall paul paule paull peal peale pearl pearl..\n",
      "P-N paign pain paine pan pane pawn payne peine pen penh penn pin pine pinn..\n",
      "P-Z pais paiz pao's pas pause paws pays paz peas pease pei's perz pez pies..\n",
      "P-T pait pat pate patt peart peat peet peete pert pet pete pett piet piett..\n",
      "P-CH patch pautsch peach perch petsch petsche piche piech pietsch pitch pit..\n",
      "P-UW1 peru peugh pew plew plue prew pru prue prugh pshew pugh..\n"
     ]
    }
   ],
   "source": [
    "p3 = [(pron[0]+'-'+pron[2], word) for (word, pron) in entries if pron[0]=='P' and len(pron)==3]\n",
    "cfd=nltk.ConditionalFreqDist(p3)\n",
    "for template in cfd.conditions():\n",
    "    if len(cfd[template])>10:\n",
    "        words=cfd[template].keys()\n",
    "        wordlist=\" \".join(words)\n",
    "        print(template, wordlist[:70]+\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('P-AA1', 'pla'),\n",
       " ('P-AH0', 'pera'),\n",
       " ('P-AH0', 'pia'),\n",
       " ('P-AW1', 'plough'),\n",
       " ('P-AW1', 'plow'),\n",
       " ('P-AW1', 'prough'),\n",
       " ('P-AW1', 'prow'),\n",
       " ('P-AY1', 'ply'),\n",
       " ('P-AY1', 'pri'),\n",
       " ('P-AY1', 'pry'),\n",
       " ('P-B', 'pub'),\n",
       " ('P-CH', 'patch'),\n",
       " ('P-CH', 'pautsch'),\n",
       " ('P-CH', 'peach'),\n",
       " ('P-CH', 'perch'),\n",
       " ('P-CH', 'petsch'),\n",
       " ('P-CH', 'petsche'),\n",
       " ('P-CH', 'piche'),\n",
       " ('P-CH', 'piech'),\n",
       " ('P-CH', 'pietsch'),\n",
       " ('P-CH', 'pitch'),\n",
       " ('P-CH', 'pitsch'),\n",
       " ('P-CH', 'poach'),\n",
       " ('P-CH', 'poche'),\n",
       " ('P-CH', 'pooch'),\n",
       " ('P-CH', 'pouch'),\n",
       " ('P-CH', 'puche'),\n",
       " ('P-CH', 'putsch'),\n",
       " ('P-D', 'pad'),\n",
       " ('P-D', 'paid'),\n",
       " ('P-D', 'pawed'),\n",
       " ('P-D', 'peed'),\n",
       " ('P-D', 'pied'),\n",
       " ('P-D', 'pod'),\n",
       " ('P-D', 'poohed'),\n",
       " ('P-ER0', 'payer'),\n",
       " ('P-ER0', 'poer'),\n",
       " ('P-ER0', 'power'),\n",
       " ('P-ER0', 'poyer'),\n",
       " ('P-ER0', 'pyre'),\n",
       " ('P-ER1', 'payeur'),\n",
       " ('P-EY1', 'play'),\n",
       " ('P-EY1', 'pray'),\n",
       " ('P-EY1', 'prey'),\n",
       " ('P-F', 'paff'),\n",
       " ('P-F', 'poff'),\n",
       " ('P-F', 'poof'),\n",
       " ('P-F', 'puff'),\n",
       " ('P-G', 'peg'),\n",
       " ('P-G', 'pegg'),\n",
       " ('P-G', 'pig'),\n",
       " ('P-G', 'pigg'),\n",
       " ('P-G', 'pigue'),\n",
       " ('P-G', 'poag'),\n",
       " ('P-G', 'pog'),\n",
       " ('P-G', 'pogue'),\n",
       " ('P-G', 'pug'),\n",
       " ('P-G', 'puig'),\n",
       " ('P-IY0', 'pai'),\n",
       " ('P-IY1', 'plea'),\n",
       " ('P-IY1', 'pre'),\n",
       " ('P-IY1', 'pree'),\n",
       " ('P-IY1', 'pri'),\n",
       " ('P-IY1', 'prix'),\n",
       " ('P-JH', 'page'),\n",
       " ('P-JH', 'paige'),\n",
       " ('P-JH', 'podge'),\n",
       " ('P-JH', 'purge'),\n",
       " ('P-K', 'pac'),\n",
       " ('P-K', 'pack'),\n",
       " ('P-K', 'paek'),\n",
       " ('P-K', 'paik'),\n",
       " ('P-K', 'pak'),\n",
       " ('P-K', 'pake'),\n",
       " ('P-K', 'paque'),\n",
       " ('P-K', 'peak'),\n",
       " ('P-K', 'peake'),\n",
       " ('P-K', 'pech'),\n",
       " ('P-K', 'peck'),\n",
       " ('P-K', 'peek'),\n",
       " ('P-K', 'perc'),\n",
       " ('P-K', 'perk'),\n",
       " ('P-K', 'pic'),\n",
       " ('P-K', 'pick'),\n",
       " ('P-K', 'pik'),\n",
       " ('P-K', 'pike'),\n",
       " ('P-K', 'pique'),\n",
       " ('P-K', 'poch'),\n",
       " ('P-K', 'pock'),\n",
       " ('P-K', 'poke'),\n",
       " ('P-K', 'polk'),\n",
       " ('P-K', 'puck'),\n",
       " ('P-K', 'purk'),\n",
       " ('P-K', 'pyke'),\n",
       " ('P-L', 'pahl'),\n",
       " ('P-L', 'pail'),\n",
       " ('P-L', 'paille'),\n",
       " ('P-L', 'pal'),\n",
       " ('P-L', 'pale'),\n",
       " ('P-L', 'pall'),\n",
       " ('P-L', 'pall'),\n",
       " ('P-L', 'paul'),\n",
       " ('P-L', 'paule'),\n",
       " ('P-L', 'paull'),\n",
       " ('P-L', 'peal'),\n",
       " ('P-L', 'peale'),\n",
       " ('P-L', 'pearl'),\n",
       " ('P-L', 'pearle'),\n",
       " ('P-L', 'peel'),\n",
       " ('P-L', 'peele'),\n",
       " ('P-L', 'pehl'),\n",
       " ('P-L', 'peil'),\n",
       " ('P-L', 'pell'),\n",
       " ('P-L', 'pelle'),\n",
       " ('P-L', 'perl'),\n",
       " ('P-L', 'perle'),\n",
       " ('P-L', 'piehl'),\n",
       " ('P-L', 'piel'),\n",
       " ('P-L', 'pihl'),\n",
       " ('P-L', 'pil'),\n",
       " ('P-L', 'pile'),\n",
       " ('P-L', 'pill'),\n",
       " ('P-L', 'pille'),\n",
       " ('P-L', 'poehl'),\n",
       " ('P-L', 'pohl'),\n",
       " ('P-L', 'pol'),\n",
       " ('P-L', 'pole'),\n",
       " ('P-L', 'poll'),\n",
       " ('P-L', 'pool'),\n",
       " ('P-L', 'poole'),\n",
       " ('P-L', 'poul'),\n",
       " ('P-L', 'puhl'),\n",
       " ('P-L', 'pull'),\n",
       " ('P-L', 'pyle'),\n",
       " ('P-M', 'palm'),\n",
       " ('P-M', 'palme'),\n",
       " ('P-M', 'pam'),\n",
       " ('P-M', 'pimm'),\n",
       " ('P-M', 'pom'),\n",
       " ('P-M', 'pymm'),\n",
       " ('P-N', 'paign'),\n",
       " ('P-N', 'pain'),\n",
       " ('P-N', 'paine'),\n",
       " ('P-N', 'pan'),\n",
       " ('P-N', 'pane'),\n",
       " ('P-N', 'pawn'),\n",
       " ('P-N', 'payne'),\n",
       " ('P-N', 'peine'),\n",
       " ('P-N', 'pen'),\n",
       " ('P-N', 'penh'),\n",
       " ('P-N', 'penn'),\n",
       " ('P-N', 'pin'),\n",
       " ('P-N', 'pine'),\n",
       " ('P-N', 'pinn'),\n",
       " ('P-N', 'pon'),\n",
       " ('P-N', 'poon'),\n",
       " ('P-N', 'pun'),\n",
       " ('P-N', 'pyne'),\n",
       " ('P-NG', 'pang'),\n",
       " ('P-NG', 'peng'),\n",
       " ('P-NG', 'ping'),\n",
       " ('P-NG', 'pong'),\n",
       " ('P-NG', 'pung'),\n",
       " ('P-OW0', 'pero'),\n",
       " ('P-OW0', 'pio'),\n",
       " ('P-OW1', 'perot'),\n",
       " ('P-OW1', 'plough'),\n",
       " ('P-OW1', 'pro'),\n",
       " ('P-OY1', 'ploy'),\n",
       " ('P-P', 'paap'),\n",
       " ('P-P', 'paape'),\n",
       " ('P-P', 'pap'),\n",
       " ('P-P', 'pape'),\n",
       " ('P-P', 'papp'),\n",
       " ('P-P', 'paup'),\n",
       " ('P-P', 'peep'),\n",
       " ('P-P', 'pep'),\n",
       " ('P-P', 'pip'),\n",
       " ('P-P', 'pipe'),\n",
       " ('P-P', 'pipp'),\n",
       " ('P-P', 'poop'),\n",
       " ('P-P', 'pop'),\n",
       " ('P-P', 'pope'),\n",
       " ('P-P', 'popp'),\n",
       " ('P-P', 'poppe'),\n",
       " ('P-P', 'pup'),\n",
       " ('P-R', 'paar'),\n",
       " ('P-R', 'pair'),\n",
       " ('P-R', 'par'),\n",
       " ('P-R', 'pare'),\n",
       " ('P-R', 'parr'),\n",
       " ('P-R', 'pear'),\n",
       " ('P-R', 'peer'),\n",
       " ('P-R', 'pier'),\n",
       " ('P-R', 'poor'),\n",
       " ('P-R', 'poore'),\n",
       " ('P-R', 'por'),\n",
       " ('P-R', 'pore'),\n",
       " ('P-R', 'porr'),\n",
       " ('P-R', 'pour'),\n",
       " ('P-S', 'pace'),\n",
       " ('P-S', 'pass'),\n",
       " ('P-S', 'pasts'),\n",
       " ('P-S', 'peace'),\n",
       " ('P-S', 'pearse'),\n",
       " ('P-S', 'pease'),\n",
       " ('P-S', 'perce'),\n",
       " ('P-S', 'pers'),\n",
       " ('P-S', 'perse'),\n",
       " ('P-S', 'pesce'),\n",
       " ('P-S', 'piece'),\n",
       " ('P-S', 'piss'),\n",
       " ('P-S', 'pos'),\n",
       " ('P-S', 'poss'),\n",
       " ('P-S', 'posts'),\n",
       " ('P-S', 'purse'),\n",
       " ('P-S', 'pus'),\n",
       " ('P-S', 'puss'),\n",
       " ('P-S', 'puss'),\n",
       " ('P-SH', 'paasch'),\n",
       " ('P-SH', 'pash'),\n",
       " ('P-SH', 'pesch'),\n",
       " ('P-SH', 'pesh'),\n",
       " ('P-SH', 'posch'),\n",
       " ('P-SH', 'posh'),\n",
       " ('P-SH', 'pusch'),\n",
       " ('P-SH', 'push'),\n",
       " ('P-T', 'pait'),\n",
       " ('P-T', 'pat'),\n",
       " ('P-T', 'pate'),\n",
       " ('P-T', 'patt'),\n",
       " ('P-T', 'peart'),\n",
       " ('P-T', 'peat'),\n",
       " ('P-T', 'peet'),\n",
       " ('P-T', 'peete'),\n",
       " ('P-T', 'pert'),\n",
       " ('P-T', 'pet'),\n",
       " ('P-T', 'pete'),\n",
       " ('P-T', 'pett'),\n",
       " ('P-T', 'piet'),\n",
       " ('P-T', 'piette'),\n",
       " ('P-T', 'pit'),\n",
       " ('P-T', 'pitt'),\n",
       " ('P-T', 'pot'),\n",
       " ('P-T', 'pote'),\n",
       " ('P-T', 'pott'),\n",
       " ('P-T', 'pout'),\n",
       " ('P-T', 'puett'),\n",
       " ('P-T', 'purt'),\n",
       " ('P-T', 'put'),\n",
       " ('P-T', 'putt'),\n",
       " ('P-TH', 'paeth'),\n",
       " ('P-TH', 'path'),\n",
       " ('P-TH', 'pathe'),\n",
       " ('P-TH', 'perth'),\n",
       " ('P-TH', 'peth'),\n",
       " ('P-TH', 'pith'),\n",
       " ('P-TH', 'poth'),\n",
       " ('P-TH', 'puth'),\n",
       " ('P-UW1', 'peru'),\n",
       " ('P-UW1', 'peugh'),\n",
       " ('P-UW1', 'pew'),\n",
       " ('P-UW1', 'plew'),\n",
       " ('P-UW1', 'plue'),\n",
       " ('P-UW1', 'prew'),\n",
       " ('P-UW1', 'pru'),\n",
       " ('P-UW1', 'prue'),\n",
       " ('P-UW1', 'prugh'),\n",
       " ('P-UW1', 'pshew'),\n",
       " ('P-UW1', 'pugh'),\n",
       " ('P-V', 'pave'),\n",
       " ('P-V', 'peeve'),\n",
       " ('P-Z', \"p's\"),\n",
       " ('P-Z', \"p.'s\"),\n",
       " ('P-Z', 'p.s'),\n",
       " ('P-Z', 'pais'),\n",
       " ('P-Z', 'paiz'),\n",
       " ('P-Z', \"pao's\"),\n",
       " ('P-Z', 'pas'),\n",
       " ('P-Z', 'pause'),\n",
       " ('P-Z', 'paws'),\n",
       " ('P-Z', 'pays'),\n",
       " ('P-Z', 'paz'),\n",
       " ('P-Z', 'peas'),\n",
       " ('P-Z', 'pease'),\n",
       " ('P-Z', \"pei's\"),\n",
       " ('P-Z', 'perz'),\n",
       " ('P-Z', 'pez'),\n",
       " ('P-Z', 'pies'),\n",
       " ('P-Z', \"poe's\"),\n",
       " ('P-Z', 'poise'),\n",
       " ('P-Z', 'pose'),\n",
       " ('P-Z', 'pows'),\n",
       " ('P-Z', 'purrs')]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(p3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConditionalFreqDist(nltk.probability.FreqDist,\n",
       "                    {'P-AA1': FreqDist({'pla': 1}),\n",
       "                     'P-AH0': FreqDist({'pera': 1, 'pia': 1}),\n",
       "                     'P-AW1': FreqDist({'plough': 1,\n",
       "                               'plow': 1,\n",
       "                               'prough': 1,\n",
       "                               'prow': 1}),\n",
       "                     'P-AY1': FreqDist({'ply': 1, 'pri': 1, 'pry': 1}),\n",
       "                     'P-B': FreqDist({'pub': 1}),\n",
       "                     'P-CH': FreqDist({'patch': 1,\n",
       "                               'pautsch': 1,\n",
       "                               'peach': 1,\n",
       "                               'perch': 1,\n",
       "                               'petsch': 1,\n",
       "                               'petsche': 1,\n",
       "                               'piche': 1,\n",
       "                               'piech': 1,\n",
       "                               'pietsch': 1,\n",
       "                               'pitch': 1,\n",
       "                               'pitsch': 1,\n",
       "                               'poach': 1,\n",
       "                               'poche': 1,\n",
       "                               'pooch': 1,\n",
       "                               'pouch': 1,\n",
       "                               'puche': 1,\n",
       "                               'putsch': 1}),\n",
       "                     'P-D': FreqDist({'pad': 1,\n",
       "                               'paid': 1,\n",
       "                               'pawed': 1,\n",
       "                               'peed': 1,\n",
       "                               'pied': 1,\n",
       "                               'pod': 1,\n",
       "                               'poohed': 1}),\n",
       "                     'P-ER0': FreqDist({'payer': 1,\n",
       "                               'poer': 1,\n",
       "                               'power': 1,\n",
       "                               'poyer': 1,\n",
       "                               'pyre': 1}),\n",
       "                     'P-ER1': FreqDist({'payeur': 1}),\n",
       "                     'P-EY1': FreqDist({'play': 1, 'pray': 1, 'prey': 1}),\n",
       "                     'P-F': FreqDist({'paff': 1,\n",
       "                               'poff': 1,\n",
       "                               'poof': 1,\n",
       "                               'puff': 1}),\n",
       "                     'P-G': FreqDist({'peg': 1,\n",
       "                               'pegg': 1,\n",
       "                               'pig': 1,\n",
       "                               'pigg': 1,\n",
       "                               'pigue': 1,\n",
       "                               'poag': 1,\n",
       "                               'pog': 1,\n",
       "                               'pogue': 1,\n",
       "                               'pug': 1,\n",
       "                               'puig': 1}),\n",
       "                     'P-IY0': FreqDist({'pai': 1}),\n",
       "                     'P-IY1': FreqDist({'plea': 1,\n",
       "                               'pre': 1,\n",
       "                               'pree': 1,\n",
       "                               'pri': 1,\n",
       "                               'prix': 1}),\n",
       "                     'P-JH': FreqDist({'page': 1,\n",
       "                               'paige': 1,\n",
       "                               'podge': 1,\n",
       "                               'purge': 1}),\n",
       "                     'P-K': FreqDist({'pac': 1,\n",
       "                               'pack': 1,\n",
       "                               'paek': 1,\n",
       "                               'paik': 1,\n",
       "                               'pak': 1,\n",
       "                               'pake': 1,\n",
       "                               'paque': 1,\n",
       "                               'peak': 1,\n",
       "                               'peake': 1,\n",
       "                               'pech': 1,\n",
       "                               'peck': 1,\n",
       "                               'peek': 1,\n",
       "                               'perc': 1,\n",
       "                               'perk': 1,\n",
       "                               'pic': 1,\n",
       "                               'pick': 1,\n",
       "                               'pik': 1,\n",
       "                               'pike': 1,\n",
       "                               'pique': 1,\n",
       "                               'poch': 1,\n",
       "                               'pock': 1,\n",
       "                               'poke': 1,\n",
       "                               'polk': 1,\n",
       "                               'puck': 1,\n",
       "                               'purk': 1,\n",
       "                               'pyke': 1}),\n",
       "                     'P-L': FreqDist({'pahl': 1,\n",
       "                               'pail': 1,\n",
       "                               'paille': 1,\n",
       "                               'pal': 1,\n",
       "                               'pale': 1,\n",
       "                               'pall': 2,\n",
       "                               'paul': 1,\n",
       "                               'paule': 1,\n",
       "                               'paull': 1,\n",
       "                               'peal': 1,\n",
       "                               'peale': 1,\n",
       "                               'pearl': 1,\n",
       "                               'pearle': 1,\n",
       "                               'peel': 1,\n",
       "                               'peele': 1,\n",
       "                               'pehl': 1,\n",
       "                               'peil': 1,\n",
       "                               'pell': 1,\n",
       "                               'pelle': 1,\n",
       "                               'perl': 1,\n",
       "                               'perle': 1,\n",
       "                               'piehl': 1,\n",
       "                               'piel': 1,\n",
       "                               'pihl': 1,\n",
       "                               'pil': 1,\n",
       "                               'pile': 1,\n",
       "                               'pill': 1,\n",
       "                               'pille': 1,\n",
       "                               'poehl': 1,\n",
       "                               'pohl': 1,\n",
       "                               'pol': 1,\n",
       "                               'pole': 1,\n",
       "                               'poll': 1,\n",
       "                               'pool': 1,\n",
       "                               'poole': 1,\n",
       "                               'poul': 1,\n",
       "                               'puhl': 1,\n",
       "                               'pull': 1,\n",
       "                               'pyle': 1}),\n",
       "                     'P-M': FreqDist({'palm': 1,\n",
       "                               'palme': 1,\n",
       "                               'pam': 1,\n",
       "                               'pimm': 1,\n",
       "                               'pom': 1,\n",
       "                               'pymm': 1}),\n",
       "                     'P-N': FreqDist({'paign': 1,\n",
       "                               'pain': 1,\n",
       "                               'paine': 1,\n",
       "                               'pan': 1,\n",
       "                               'pane': 1,\n",
       "                               'pawn': 1,\n",
       "                               'payne': 1,\n",
       "                               'peine': 1,\n",
       "                               'pen': 1,\n",
       "                               'penh': 1,\n",
       "                               'penn': 1,\n",
       "                               'pin': 1,\n",
       "                               'pine': 1,\n",
       "                               'pinn': 1,\n",
       "                               'pon': 1,\n",
       "                               'poon': 1,\n",
       "                               'pun': 1,\n",
       "                               'pyne': 1}),\n",
       "                     'P-NG': FreqDist({'pang': 1,\n",
       "                               'peng': 1,\n",
       "                               'ping': 1,\n",
       "                               'pong': 1,\n",
       "                               'pung': 1}),\n",
       "                     'P-OW0': FreqDist({'pero': 1, 'pio': 1}),\n",
       "                     'P-OW1': FreqDist({'perot': 1, 'plough': 1, 'pro': 1}),\n",
       "                     'P-OY1': FreqDist({'ploy': 1}),\n",
       "                     'P-P': FreqDist({'paap': 1,\n",
       "                               'paape': 1,\n",
       "                               'pap': 1,\n",
       "                               'pape': 1,\n",
       "                               'papp': 1,\n",
       "                               'paup': 1,\n",
       "                               'peep': 1,\n",
       "                               'pep': 1,\n",
       "                               'pip': 1,\n",
       "                               'pipe': 1,\n",
       "                               'pipp': 1,\n",
       "                               'poop': 1,\n",
       "                               'pop': 1,\n",
       "                               'pope': 1,\n",
       "                               'popp': 1,\n",
       "                               'poppe': 1,\n",
       "                               'pup': 1}),\n",
       "                     'P-R': FreqDist({'paar': 1,\n",
       "                               'pair': 1,\n",
       "                               'par': 1,\n",
       "                               'pare': 1,\n",
       "                               'parr': 1,\n",
       "                               'pear': 1,\n",
       "                               'peer': 1,\n",
       "                               'pier': 1,\n",
       "                               'poor': 1,\n",
       "                               'poore': 1,\n",
       "                               'por': 1,\n",
       "                               'pore': 1,\n",
       "                               'porr': 1,\n",
       "                               'pour': 1}),\n",
       "                     'P-S': FreqDist({'pace': 1,\n",
       "                               'pass': 1,\n",
       "                               'pasts': 1,\n",
       "                               'peace': 1,\n",
       "                               'pearse': 1,\n",
       "                               'pease': 1,\n",
       "                               'perce': 1,\n",
       "                               'pers': 1,\n",
       "                               'perse': 1,\n",
       "                               'pesce': 1,\n",
       "                               'piece': 1,\n",
       "                               'piss': 1,\n",
       "                               'pos': 1,\n",
       "                               'poss': 1,\n",
       "                               'posts': 1,\n",
       "                               'purse': 1,\n",
       "                               'pus': 1,\n",
       "                               'puss': 2}),\n",
       "                     'P-SH': FreqDist({'paasch': 1,\n",
       "                               'pash': 1,\n",
       "                               'pesch': 1,\n",
       "                               'pesh': 1,\n",
       "                               'posch': 1,\n",
       "                               'posh': 1,\n",
       "                               'pusch': 1,\n",
       "                               'push': 1}),\n",
       "                     'P-T': FreqDist({'pait': 1,\n",
       "                               'pat': 1,\n",
       "                               'pate': 1,\n",
       "                               'patt': 1,\n",
       "                               'peart': 1,\n",
       "                               'peat': 1,\n",
       "                               'peet': 1,\n",
       "                               'peete': 1,\n",
       "                               'pert': 1,\n",
       "                               'pet': 1,\n",
       "                               'pete': 1,\n",
       "                               'pett': 1,\n",
       "                               'piet': 1,\n",
       "                               'piette': 1,\n",
       "                               'pit': 1,\n",
       "                               'pitt': 1,\n",
       "                               'pot': 1,\n",
       "                               'pote': 1,\n",
       "                               'pott': 1,\n",
       "                               'pout': 1,\n",
       "                               'puett': 1,\n",
       "                               'purt': 1,\n",
       "                               'put': 1,\n",
       "                               'putt': 1}),\n",
       "                     'P-TH': FreqDist({'paeth': 1,\n",
       "                               'path': 1,\n",
       "                               'pathe': 1,\n",
       "                               'perth': 1,\n",
       "                               'peth': 1,\n",
       "                               'pith': 1,\n",
       "                               'poth': 1,\n",
       "                               'puth': 1}),\n",
       "                     'P-UW1': FreqDist({'peru': 1,\n",
       "                               'peugh': 1,\n",
       "                               'pew': 1,\n",
       "                               'plew': 1,\n",
       "                               'plue': 1,\n",
       "                               'prew': 1,\n",
       "                               'pru': 1,\n",
       "                               'prue': 1,\n",
       "                               'prugh': 1,\n",
       "                               'pshew': 1,\n",
       "                               'pugh': 1}),\n",
       "                     'P-V': FreqDist({'pave': 1, 'peeve': 1}),\n",
       "                     'P-Z': FreqDist({\"p's\": 1,\n",
       "                               \"p.'s\": 1,\n",
       "                               'p.s': 1,\n",
       "                               'pais': 1,\n",
       "                               'paiz': 1,\n",
       "                               \"pao's\": 1,\n",
       "                               'pas': 1,\n",
       "                               'pause': 1,\n",
       "                               'paws': 1,\n",
       "                               'pays': 1,\n",
       "                               'paz': 1,\n",
       "                               'peas': 1,\n",
       "                               'pease': 1,\n",
       "                               \"pei's\": 1,\n",
       "                               'perz': 1,\n",
       "                               'pez': 1,\n",
       "                               'pies': 1,\n",
       "                               \"poe's\": 1,\n",
       "                               'poise': 1,\n",
       "                               'pose': 1,\n",
       "                               'pows': 1,\n",
       "                               'purrs': 1})})"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['F', 'AY1', 'ER0'], ['F', 'AY1', 'R']]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prondict = nltk.corpus.cmudict.dict()\n",
    "prondict['fire']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n"
     ]
    }
   ],
   "source": [
    "print(prondict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['N', 'AE1', 'CH', 'ER0', 'AH0', 'L', 'L', 'AE1', 'NG', 'G', 'W', 'AH0', 'JH', 'P', 'R', 'AA1', 'S', 'EH0', 'S', 'IH0', 'NG']\n"
     ]
    }
   ],
   "source": [
    "text = ['natural', 'language', 'processing']\n",
    "print([ph for w in text for ph in prondict[w][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['N', 'AE1', 'CH', 'ER0', 'AH0', 'L']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prondict['natural'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparative Wordlists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['be', 'bg', 'bs', 'ca', 'cs', 'cu', 'de', 'en', 'es', 'fr', 'hr', 'it', 'la', 'mk', 'nl', 'pl', 'pt', 'ro', 'ru', 'sk', 'sl', 'sr', 'sw', 'uk']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import swadesh\n",
    "print(swadesh.fileids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'you (singular), thou', 'he', 'we', 'you (plural)', 'they', 'this', 'that', 'here', 'there', 'who', 'what', 'where', 'when', 'how', 'not', 'all', 'many', 'some', 'few', 'other', 'one', 'two', 'three', 'four', 'five', 'big', 'long', 'wide', 'thick', 'heavy', 'small', 'short', 'narrow', 'thin', 'woman', 'man (adult male)', 'man (human being)', 'child', 'wife', 'husband', 'mother', 'father', 'animal', 'fish', 'bird', 'dog', 'louse', 'snake', 'worm', 'tree', 'forest', 'stick', 'fruit', 'seed', 'leaf', 'root', 'bark (from tree)', 'flower', 'grass', 'rope', 'skin', 'meat', 'blood', 'bone', 'fat (noun)', 'egg', 'horn', 'tail', 'feather', 'hair', 'head', 'ear', 'eye', 'nose', 'mouth', 'tooth', 'tongue', 'fingernail', 'foot', 'leg', 'knee', 'hand', 'wing', 'belly', 'guts', 'neck', 'back', 'breast', 'heart', 'liver', 'drink', 'eat', 'bite', 'suck', 'spit', 'vomit', 'blow', 'breathe', 'laugh', 'see', 'hear', 'know (a fact)', 'think', 'smell', 'fear', 'sleep', 'live', 'die', 'kill', 'fight', 'hunt', 'hit', 'cut', 'split', 'stab', 'scratch', 'dig', 'swim', 'fly (verb)', 'walk', 'come', 'lie', 'sit', 'stand', 'turn', 'fall', 'give', 'hold', 'squeeze', 'rub', 'wash', 'wipe', 'pull', 'push', 'throw', 'tie', 'sew', 'count', 'say', 'sing', 'play', 'float', 'flow', 'freeze', 'swell', 'sun', 'moon', 'star', 'water', 'rain', 'river', 'lake', 'sea', 'salt', 'stone', 'sand', 'dust', 'earth', 'cloud', 'fog', 'sky', 'wind', 'snow', 'ice', 'smoke', 'fire', 'ashes', 'burn', 'road', 'mountain', 'red', 'green', 'yellow', 'white', 'black', 'night', 'day', 'year', 'warm', 'cold', 'full', 'new', 'old', 'good', 'bad', 'rotten', 'dirty', 'straight', 'round', 'sharp', 'dull', 'smooth', 'wet', 'dry', 'correct', 'near', 'far', 'right', 'left', 'at', 'in', 'with', 'and', 'if', 'because', 'name']\n"
     ]
    }
   ],
   "source": [
    "print(swadesh.words('en'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### We can find cognate (similar) words in multiple languages using entries() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 'je'), ('you (singular), thou', 'tu, vous'), ('he', 'il'), ('we', 'nous'), ('you (plural)', 'vous'), ('they', 'ils, elles'), ('this', 'ceci'), ('that', 'cela'), ('here', 'ici'), ('there', 'l'), ('who', 'qui'), ('what', 'quoi'), ('where', 'o'), ('when', 'quand'), ('how', 'comment'), ('not', 'ne...pas'), ('all', 'tout'), ('many', 'plusieurs'), ('some', 'quelques'), ('few', 'peu'), ('other', 'autre'), ('one', 'un'), ('two', 'deux'), ('three', 'trois'), ('four', 'quatre'), ('five', 'cinq'), ('big', 'grand'), ('long', 'long'), ('wide', 'large'), ('thick', 'pais'), ('heavy', 'lourd'), ('small', 'petit'), ('short', 'court'), ('narrow', 'troit'), ('thin', 'mince'), ('woman', 'femme'), ('man (adult male)', 'homme'), ('man (human being)', 'homme'), ('child', 'enfant'), ('wife', 'femme, pouse'), ('husband', 'mari, poux'), ('mother', 'mre'), ('father', 'pre'), ('animal', 'animal'), ('fish', 'poisson'), ('bird', 'oiseau'), ('dog', 'chien'), ('louse', 'pou'), ('snake', 'serpent'), ('worm', 'ver'), ('tree', 'arbre'), ('forest', 'fort'), ('stick', 'bton'), ('fruit', 'fruit'), ('seed', 'graine'), ('leaf', 'feuille'), ('root', 'racine'), ('bark (from tree)', 'corce'), ('flower', 'fleur'), ('grass', 'herbe'), ('rope', 'corde'), ('skin', 'peau'), ('meat', 'viande'), ('blood', 'sang'), ('bone', 'os'), ('fat (noun)', 'graisse'), ('egg', 'uf'), ('horn', 'corne'), ('tail', 'queue'), ('feather', 'plume'), ('hair', 'cheveu'), ('head', 'tte'), ('ear', 'oreille'), ('eye', 'il'), ('nose', 'nez'), ('mouth', 'bouche'), ('tooth', 'dent'), ('tongue', 'langue'), ('fingernail', 'ongle'), ('foot', 'pied'), ('leg', 'jambe'), ('knee', 'genou'), ('hand', 'main'), ('wing', 'aile'), ('belly', 'ventre'), ('guts', 'entrailles'), ('neck', 'cou'), ('back', 'dos'), ('breast', 'sein, poitrine'), ('heart', 'cur'), ('liver', 'foie'), ('drink', 'boire'), ('eat', 'manger'), ('bite', 'mordre'), ('suck', 'sucer'), ('spit', 'cracher'), ('vomit', 'vomir'), ('blow', 'souffler'), ('breathe', 'respirer'), ('laugh', 'rire'), ('see', 'voir'), ('hear', 'entendre'), ('know (a fact)', 'savoir'), ('think', 'penser'), ('smell', 'sentir'), ('fear', 'craindre, avoir peur'), ('sleep', 'dormir'), ('live', 'vivre'), ('die', 'mourir'), ('kill', 'tuer'), ('fight', 'se battre'), ('hunt', 'chasser'), ('hit', 'frapper'), ('cut', 'couper'), ('split', 'fendre'), ('stab', 'poignarder'), ('scratch', 'gratter'), ('dig', 'creuser'), ('swim', 'nager'), ('fly (verb)', 'voler'), ('walk', 'marcher'), ('come', 'venir'), ('lie', \"s'tendre\"), ('sit', \"s'asseoir\"), ('stand', 'se lever'), ('turn', 'tourner'), ('fall', 'tomber'), ('give', 'donner'), ('hold', 'tenir'), ('squeeze', 'serrer'), ('rub', 'frotter'), ('wash', 'laver'), ('wipe', 'essuyer'), ('pull', 'tirer'), ('push', 'pousser'), ('throw', 'jeter'), ('tie', 'lier'), ('sew', 'coudre'), ('count', 'compter'), ('say', 'dire'), ('sing', 'chanter'), ('play', 'jouer'), ('float', 'flotter'), ('flow', 'couler'), ('freeze', 'geler'), ('swell', 'gonfler'), ('sun', 'soleil'), ('moon', 'lune'), ('star', 'toile'), ('water', 'eau'), ('rain', 'pluie'), ('river', 'rivire'), ('lake', 'lac'), ('sea', 'mer'), ('salt', 'sel'), ('stone', 'pierre'), ('sand', 'sable'), ('dust', 'poussire'), ('earth', 'terre'), ('cloud', 'nuage'), ('fog', 'brouillard'), ('sky', 'ciel'), ('wind', 'vent'), ('snow', 'neige'), ('ice', 'glace'), ('smoke', 'fume'), ('fire', 'feu'), ('ashes', 'cendres'), ('burn', 'brler'), ('road', 'route'), ('mountain', 'montagne'), ('red', 'rouge'), ('green', 'vert'), ('yellow', 'jaune'), ('white', 'blanc'), ('black', 'noir'), ('night', 'nuit'), ('day', 'jour'), ('year', 'an, anne'), ('warm', 'chaud'), ('cold', 'froid'), ('full', 'plein'), ('new', 'nouveau'), ('old', 'vieux'), ('good', 'bon'), ('bad', 'mauvais'), ('rotten', 'pourri'), ('dirty', 'sale'), ('straight', 'droit'), ('round', 'rond'), ('sharp', 'tranchant, pointu, aigu'), ('dull', 'mouss'), ('smooth', 'lisse'), ('wet', 'mouill'), ('dry', 'sec'), ('correct', 'juste, correct'), ('near', 'proche'), ('far', 'loin'), ('right', ' droite'), ('left', ' gauche'), ('at', ''), ('in', 'dans'), ('with', 'avec'), ('and', 'et'), ('if', 'si'), ('because', 'parce que'), ('name', 'nom')]\n"
     ]
    }
   ],
   "source": [
    "fr2en=swadesh.entries(['fr','en'])#French and English\n",
    "print(fr2en)                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nom'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate = dict(fr2en)\n",
    "translate['name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WordNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Semantically oriented dictionary of English."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Senses and Synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('car.n.01')]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "wn.synsets('motorcar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above answer means that the word 'motorcar' has just one possible meaning, the forst noun set of car synset means a synonymset(collection of synonyms). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['car', 'auto', 'automobile', 'machine', 'motorcar']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('car.n.01').lemma_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each word of synset can have a different meaning. However we're intersted in the common meaning in the above synset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a motor vehicle with four wheels; usually propelled by an internal combustion engine'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('car.n.01').definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['he needs a car to get to work']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('car.n.01').examples()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To eliminate ambiguity we will identify these words as car.n.01.automobile and car.n.01.motorcar and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Lemma('car.n.01.car'),\n",
       " Lemma('car.n.01.auto'),\n",
       " Lemma('car.n.01.automobile'),\n",
       " Lemma('car.n.01.machine'),\n",
       " Lemma('car.n.01.motorcar')]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('car.n.01').lemmas() #The pairing of a synset is lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'automobile'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.lemma('car.n.01.automobile').name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Synset('car.n.01')"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.lemma('car.n.01.automobile').synset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('car.n.01'),\n",
       " Synset('car.n.02'),\n",
       " Synset('car.n.03'),\n",
       " Synset('car.n.04'),\n",
       " Synset('cable_car.n.01')]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets('car') #The word car is ambiguous "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('car.n.01')]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets('motorcar') #The word motorcar is unambiguous "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['car', 'auto', 'automobile', 'machine', 'motorcar']\n",
      "['car', 'railcar', 'railway_car', 'railroad_car']\n",
      "['car', 'gondola']\n",
      "['car', 'elevator_car']\n",
      "['cable_car', 'car']\n"
     ]
    }
   ],
   "source": [
    "for syn in wn.synsets('car'):\n",
    "    print(syn.lemma_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Lemma('car.n.01.car'),\n",
       " Lemma('car.n.02.car'),\n",
       " Lemma('car.n.03.car'),\n",
       " Lemma('car.n.04.car'),\n",
       " Lemma('cable_car.n.01.car')]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.lemmas('car') #We can access all the lemmas having word 'car' using this method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('dish.n.01'),\n",
       " Synset('dish.n.02'),\n",
       " Synset('dish.n.03'),\n",
       " Synset('smasher.n.02'),\n",
       " Synset('dish.n.05'),\n",
       " Synset('cup_of_tea.n.01'),\n",
       " Synset('serve.v.06'),\n",
       " Synset('dish.v.02')]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets('dish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dish']\n",
      "['dish']\n",
      "['dish', 'dishful']\n",
      "['smasher', 'stunner', 'knockout', 'beauty', 'ravisher', 'sweetheart', 'peach', 'lulu', 'looker', 'mantrap', 'dish']\n",
      "['dish', 'dish_aerial', 'dish_antenna', 'saucer']\n",
      "['cup_of_tea', 'bag', 'dish']\n",
      "['serve', 'serve_up', 'dish_out', 'dish_up', 'dish']\n",
      "['dish']\n"
     ]
    }
   ],
   "source": [
    "for w in wn.synsets('dish'):\n",
    "    print(w.lemma_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Lemma('dish.n.01.dish'),\n",
       " Lemma('dish.n.02.dish'),\n",
       " Lemma('dish.n.03.dish'),\n",
       " Lemma('smasher.n.02.dish'),\n",
       " Lemma('dish.n.05.dish'),\n",
       " Lemma('cup_of_tea.n.01.dish'),\n",
       " Lemma('serve.v.06.dish'),\n",
       " Lemma('dish.v.02.dish')]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.lemmas('dish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dish']  : a piece of dishware normally used as a container for holding or serving food\n",
      "['dish']  : a particular item of prepared food\n",
      "['dish', 'dishful']  : the quantity that a dish will hold\n",
      "['smasher', 'stunner', 'knockout', 'beauty', 'ravisher', 'sweetheart', 'peach', 'lulu', 'looker', 'mantrap', 'dish']  : a very attractive or seductive looking woman\n",
      "['dish', 'dish_aerial', 'dish_antenna', 'saucer']  : directional antenna consisting of a parabolic reflector for microwave or radio frequency radiation\n",
      "['cup_of_tea', 'bag', 'dish']  : an activity that you like or at which you are superior\n",
      "['serve', 'serve_up', 'dish_out', 'dish_up', 'dish']  : provide (usually but not necessarily food)\n",
      "['dish']  : make concave; shape like a dish\n"
     ]
    }
   ],
   "source": [
    "for syn in wn.synsets('dish'):\n",
    "    print(syn.lemma_names(), ' : ' + syn.definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['train', 'railroad_train']  : public transport provided by a line of railway cars coupled together and drawn by a locomotive\n",
      "['string', 'train']  : a sequentially ordered set of things or events or ideas in which each successive member is related to the preceding\n",
      "['caravan', 'train', 'wagon_train']  : a procession (of wagons or mules or camels) traveling together in single file\n",
      "['train']  : a series of consequences wrought by an event\n",
      "['train']  : piece of cloth forming the long back section of a gown that is drawn along the floor\n",
      "['gearing', 'gear', 'geartrain', 'power_train', 'train']  : wheelwork consisting of a connected set of rotating gears by which force is transmitted or motion or torque is changed\n",
      "['train', 'develop', 'prepare', 'educate']  : create by training and teaching\n",
      "['train', 'prepare']  : undergo training or instruction in preparation for a particular role, function, or profession\n",
      "['discipline', 'train', 'check', 'condition']  : develop (children's) behavior by instruction and practice; especially to teach self-control\n",
      "['prepare', 'groom', 'train']  : educate for a future role or function\n",
      "['educate', 'school', 'train', 'cultivate', 'civilize', 'civilise']  : teach or refine to be discriminative in taste or judgment\n",
      "['aim', 'take', 'train', 'take_aim', 'direct']  : point or cause to go (blows, weapons, or objects such as photographic equipment) towards\n",
      "['coach', 'train']  : teach and supervise (someone); act as a trainer or coach (to), as in sports\n",
      "['train']  : exercise in order to prepare for an event or competition\n",
      "['train']  : cause to grow in a certain way by tying and pruning it\n",
      "['train', 'rail']  : travel by rail or train\n",
      "['trail', 'train']  : drag loosely along a surface; allow to sweep the ground\n"
     ]
    }
   ],
   "source": [
    "for syn in wn.synsets('train'):\n",
    "    print(syn.lemma_names(), ' : ' + syn.definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "motorcar=wn.synset('car.n.01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('fix.v.12'),\n",
       " Synset('cook.v.02'),\n",
       " Synset('prepare.v.03'),\n",
       " Synset('organize.v.05'),\n",
       " Synset('prepare.v.05'),\n",
       " Synset('train.v.01'),\n",
       " Synset('prepare.v.07'),\n",
       " Synset('train.v.02')]"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets('prepare')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('cook.n.01'),\n",
       " Synset('cook.n.02'),\n",
       " Synset('cook.v.01'),\n",
       " Synset('cook.v.02'),\n",
       " Synset('cook.v.03'),\n",
       " Synset('fudge.v.01'),\n",
       " Synset('cook.v.05')]"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets('cook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('misrepresent.v.01'), Synset('fudge.v.01')]"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets('misrepresent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cook']  : someone who cooks food\n",
      "['Cook', 'James_Cook', 'Captain_Cook', 'Captain_James_Cook']  : English navigator who claimed the east coast of Australia for Britain and discovered several Pacific islands (1728-1779)\n",
      "['cook']  : prepare a hot meal\n",
      "['cook', 'fix', 'ready', 'make', 'prepare']  : prepare for eating by applying heat\n",
      "['cook']  : transform and make suitable for consumption by heating\n",
      "['fudge', 'manipulate', 'fake', 'falsify', 'cook', 'wangle', 'misrepresent']  : tamper, with the purpose of deception\n",
      "['cook']  : transform by heating\n"
     ]
    }
   ],
   "source": [
    "for syn in wn.synsets('cook'):\n",
    "    print(syn.lemma_names(), ' : ' + syn.definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Synset('stanley_steamer.n.01')"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "motorcar=wn.synset('car.n.01')\n",
    "types_of_motorcar = motorcar.hyponyms()\n",
    "types_of_motorcar[26]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Model_T', 'S.U.V.', 'SUV', 'Stanley_Steamer', 'ambulance', 'beach_waggon', 'beach_wagon', 'bus', 'cab', 'compact', 'compact_car', 'convertible', 'coupe', 'cruiser', 'electric', 'electric_automobile', 'electric_car', 'estate_car', 'gas_guzzler', 'hack', 'hardtop', 'hatchback', 'heap', 'horseless_carriage', 'hot-rod', 'hot_rod', 'jalopy', 'jeep', 'landrover', 'limo', 'limousine', 'loaner', 'minicar', 'minivan', 'pace_car', 'patrol_car', 'phaeton', 'police_car', 'police_cruiser', 'prowl_car', 'race_car', 'racer', 'racing_car', 'roadster', 'runabout', 'saloon', 'secondhand_car', 'sedan', 'sport_car', 'sport_utility', 'sport_utility_vehicle', 'sports_car', 'squad_car', 'station_waggon', 'station_wagon', 'stock_car', 'subcompact', 'subcompact_car', 'taxi', 'taxicab', 'tourer', 'touring_car', 'two-seater', 'used-car', 'waggon', 'wagon']\n"
     ]
    }
   ],
   "source": [
    "print(sorted([lemma.name() for syn in types_of_motorcar for lemma in syn.lemmas()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('car.n.01')]"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('stanley_steamer.n.01').hypernyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('motor_vehicle.n.01')]"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('car.n.01').hypernyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths=motorcar.hypernym_paths()\n",
    "len(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Synset('entity.n.01'),\n",
       "  Synset('physical_entity.n.01'),\n",
       "  Synset('object.n.01'),\n",
       "  Synset('whole.n.02'),\n",
       "  Synset('artifact.n.01'),\n",
       "  Synset('instrumentality.n.03'),\n",
       "  Synset('container.n.01'),\n",
       "  Synset('wheeled_vehicle.n.01'),\n",
       "  Synset('self-propelled_vehicle.n.01'),\n",
       "  Synset('motor_vehicle.n.01'),\n",
       "  Synset('car.n.01')],\n",
       " [Synset('entity.n.01'),\n",
       "  Synset('physical_entity.n.01'),\n",
       "  Synset('object.n.01'),\n",
       "  Synset('whole.n.02'),\n",
       "  Synset('artifact.n.01'),\n",
       "  Synset('instrumentality.n.03'),\n",
       "  Synset('conveyance.n.03'),\n",
       "  Synset('vehicle.n.01'),\n",
       "  Synset('wheeled_vehicle.n.01'),\n",
       "  Synset('self-propelled_vehicle.n.01'),\n",
       "  Synset('motor_vehicle.n.01'),\n",
       "  Synset('car.n.01')]]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['entity.n.01',\n",
       " 'physical_entity.n.01',\n",
       " 'object.n.01',\n",
       " 'whole.n.02',\n",
       " 'artifact.n.01',\n",
       " 'instrumentality.n.03',\n",
       " 'container.n.01',\n",
       " 'wheeled_vehicle.n.01',\n",
       " 'self-propelled_vehicle.n.01',\n",
       " 'motor_vehicle.n.01',\n",
       " 'car.n.01']"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[synset.name() for synset in paths[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hypernyms and hyponyms are called lexical relations because they relate one synset\n",
    "to another. These two relations navigate up and down the is-a hierarchy. Another\n",
    "important way to navigate the WordNet network is from items to their components\n",
    "(meronyms) or to the things they are contained in (holonyms). For example, the parts\n",
    "of a tree are its trunk, crown, and so on; these are the part_meronyms(). The substance\n",
    "a tree is made of includes heartwood and sapwood, i.e., the substance_meronyms(). A\n",
    "collection of trees forms a forest, i.e., the member_holonyms():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('tree.n.01'),\n",
       " Synset('tree.n.02'),\n",
       " Synset('tree.n.03'),\n",
       " Synset('corner.v.02'),\n",
       " Synset('tree.v.02'),\n",
       " Synset('tree.v.03'),\n",
       " Synset('tree.v.04')]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets('tree')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('burl.n.02'),\n",
       " Synset('crown.n.07'),\n",
       " Synset('limb.n.02'),\n",
       " Synset('stump.n.01'),\n",
       " Synset('trunk.n.01')]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('tree.n.01').part_meronyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('heartwood.n.01'), Synset('sapwood.n.01')]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('tree.n.01').substance_meronyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('forest.n.01')]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('tree.n.01').member_holonyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('batch.n.02'),\n",
       " Synset('mint.n.02'),\n",
       " Synset('mint.n.03'),\n",
       " Synset('mint.n.04'),\n",
       " Synset('mint.n.05'),\n",
       " Synset('mint.n.06'),\n",
       " Synset('mint.v.01'),\n",
       " Synset('mint.s.01')]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets('mint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('batch.n.02'),\n",
       " Synset('mint.n.02'),\n",
       " Synset('mint.n.03'),\n",
       " Synset('mint.n.04'),\n",
       " Synset('mint.n.05'),\n",
       " Synset('mint.n.06')]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets(\"mint\", wn.NOUN) #return only synset where it is a noun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'any of the forms of Chinese spoken in Fukien province'"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('mint.n.02').definition() #Here it is a noun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'form by stamping, punching, or printing'"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('mint.v.01').definition()#Here it is not a noun henc why it was removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch.n.02 : (often followed by `of') a large number or amount or extent\n",
      "mint.n.02 : any north temperate plant of the genus Mentha with aromatic leaves and small mauve flowers\n",
      "mint.n.03 : any member of the mint family of plants\n",
      "mint.n.04 : the leaves of a mint plant used fresh or candied\n",
      "mint.n.05 : a candy that is flavored with a mint oil\n",
      "mint.n.06 : a plant where money is coined by authority of the government\n"
     ]
    }
   ],
   "source": [
    "for synset in wn.synsets('mint', wn.NOUN):\n",
    "    print(synset.name()+ ' : ' + synset.definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['batch', 'deal', 'flock', 'good_deal', 'great_deal', 'hatful', 'heap', 'lot', 'mass', 'mess', 'mickle', 'mint', 'mountain', 'muckle', 'passel', 'peck', 'pile', 'plenty', 'pot', 'quite_a_little', 'raft', 'sight', 'slew', 'spate', 'stack', 'tidy_sum', 'wad']\n"
     ]
    }
   ],
   "source": [
    "print(wn.synset('batch.n.02').lemma_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('mint.n.02')]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('mint.n.04').part_holonyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('mint.n.05')]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('mint.n.04').substance_holonyms()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are also relationships between verbs. For example, the act of walking involves\n",
    "the act of stepping, so walking entails stepping. Some verbs have multiple entailments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('step.v.01')]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('walk.v.01').entailments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('chew.v.01'), Synset('swallow.v.01')]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('eat.v.01').entailments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('arouse.v.07'), Synset('disappoint.v.01')]"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('tease.v.03').entailments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'to arouse hope, desire, or curiosity without satisfying them'"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('tease.v.03').definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tease', 'teaser', 'annoyer', 'vexer'] someone given to teasing (as by mocking or stirring curiosity)\n",
      "['coquette', 'flirt', 'vamp', 'vamper', 'minx', 'tease', 'prickteaser'] a seductive woman who uses her sex appeal to exploit men\n",
      "['tease', 'teasing', 'ribbing', 'tantalization'] the act of harassing someone playfully or maliciously (especially by ridicule); provoking someone with persistent annoyances\n",
      "['tease', 'badger', 'pester', 'bug', 'beleaguer'] annoy persistently\n",
      "['tease', 'razz', 'rag', 'cod', 'tantalize', 'tantalise', 'bait', 'taunt', 'twit', 'rally', 'ride'] harass with persistent criticism or carping\n",
      "['tease'] to arouse hope, desire, or curiosity without satisfying them\n",
      "['tease'] tear into pieces\n",
      "['tease'] raise the nap of (fabrics)\n",
      "['tease', 'tease_apart', 'loosen'] disentangle and raise the fibers of\n",
      "['tease', 'card'] separate the fibers of\n",
      "['tease'] mock or make fun of playfully\n",
      "['tease', 'fluff'] ruffle (one's hair) by combing the ends towards the scalp, for a full effect\n"
     ]
    }
   ],
   "source": [
    "for syn in wn.synsets('tease'):\n",
    "    print(syn.lemma_names(), syn.definition())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Antonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Lemma('demand.n.02.demand')]"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.lemma('supply.n.02.supply').antonyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Lemma('linger.v.04.linger')]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.lemma('rush.v.01.rush').antonyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Lemma('inclined.a.02.inclined'), Lemma('vertical.a.01.vertical')]"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.lemma('horizontal.a.01.horizontal').antonyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Lemma('legato.r.01.legato')]"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.lemma('staccato.r.01.staccato').antonyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', '__unicode__', '__weakref__', '_all_hypernyms', '_definition', '_examples', '_frame_ids', '_hypernyms', '_instance_hypernyms', '_iter_hypernym_lists', '_lemma_names', '_lemma_pointers', '_lemmas', '_lexname', '_max_depth', '_min_depth', '_name', '_needs_root', '_offset', '_pointers', '_pos', '_related', '_shortest_hypernym_paths', '_wordnet_corpus_reader', 'also_sees', 'attributes', 'causes', 'closure', 'common_hypernyms', 'definition', 'entailments', 'examples', 'frame_ids', 'hypernym_distances', 'hypernym_paths', 'hypernyms', 'hyponyms', 'instance_hypernyms', 'instance_hyponyms', 'jcn_similarity', 'lch_similarity', 'lemma_names', 'lemmas', 'lexname', 'lin_similarity', 'lowest_common_hypernyms', 'max_depth', 'member_holonyms', 'member_meronyms', 'min_depth', 'name', 'offset', 'part_holonyms', 'part_meronyms', 'path_similarity', 'pos', 'region_domains', 'res_similarity', 'root_hypernyms', 'shortest_path_distance', 'similar_tos', 'substance_holonyms', 'substance_meronyms', 'topic_domains', 'tree', 'unicode_repr', 'usage_domains', 'verb_groups', 'wup_similarity']\n"
     ]
    }
   ],
   "source": [
    "#to view the lexical relations\n",
    "print(dir(wn.synset('harmony.n.02')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('make.v.03'),\n",
       " Synset('create.v.02'),\n",
       " Synset('create.v.03'),\n",
       " Synset('create.v.04'),\n",
       " Synset('create.v.05'),\n",
       " Synset('produce.v.02')]"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets('create')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['make', 'create']  : make or cause to be or to become\n",
      "['create']  : bring into existence\n",
      "['create']  : pursue a creative activity; be engaged in a creative activity\n",
      "['create']  : invest with a new title, office, or rank\n",
      "['create', 'make']  : create by artistic means\n",
      "['produce', 'make', 'create']  : create or manufacture a man-made product\n"
     ]
    }
   ],
   "source": [
    "for syn in wn.synsets('create'):\n",
    "    print(syn.lemma_names(), ' : ' + syn.definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Lemma('make.v.03.make'), Lemma('make.v.03.create')]"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('make.v.03').lemmas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that each synset has one or more hypernym paths that link it to a root hypernym\n",
    "such as entity.n.01. Two synsets linked to the same root may have several hypernyms\n",
    "in common. If two synsets share a very specific hypernymone that\n",
    "is low down in the hypernym hierarchythey must be closely related."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('whale.n.02')]"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "right = wn.synset('right_whale.n.01')\n",
    "orca = wn.synset('orca.n.01')\n",
    "minke = wn.synset('minke_whale.n.01')\n",
    "tortoise = wn.synset('tortoise.n.01')\n",
    "novel = wn.synset('novel.n.01')\n",
    "right.lowest_common_hypernyms(orca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('baleen_whale.n.01')]"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "right.lowest_common_hypernyms(minke)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('vertebrate.n.01')]"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "right.lowest_common_hypernyms(tortoise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('entity.n.01')]"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "right.lowest_common_hypernyms(novel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course we know that whale is very specific (and baleen whale even more so), whereas\n",
    "vertebrate is more general and entity is completely general. We can quantify this concept\n",
    "of generality by looking up the depth of each synset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('whale.n.02').min_depth()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('vertebrate.n.01').min_depth()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('baleen_whale.n.01').min_depth()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('entity.n.01').min_depth()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarity measures have been defined over the collection of WordNet synsets that\n",
    "incorporate this insight. For example, path_similarity assigns a score in the range\n",
    "01 based on the shortest path that connects the concepts in the hypernym hierarchy\n",
    "(-1 is returned in those cases where a path cannot be found). Comparing a synset with\n",
    "itself will return 1. Consider the following similarity scores, relating right whale to minke\n",
    "whale, orca, tortoise, and novel. Although the numbers wont mean much, they decrease\n",
    "as we move away from the semantic space of sea creatures to inanimate objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "right.path_similarity(minke)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16666666666666666"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "right.path_similarity(orca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.043478260869565216"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "right.path_similarity(novel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### NLTK also includes VerbNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConditionalFreqDist(nltk.probability.FreqDist,\n",
       "                    {'category 0': FreqDist({'hello': 1, 'hi': 1}),\n",
       "                     'category 1': FreqDist({'Bye': 1, 'hello': 2, 'hi': 1})})"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_list=[('category 1',\"hello\"), ('category 1',\"hi\"), ('category 1',\"hello\"), ('category 0',\"hello\"),\n",
    "             ('category 0',\"hi\"),('category 1',\"Bye\")]\n",
    "cfd=nltk.ConditionalFreqDist(simple_list)\n",
    "cfd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('misrepresent.v.01'), Synset('fudge.v.01')]"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "wn.synsets('misrepresent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'represent falsely'"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('misrepresent.v.01').definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Lemma('misrepresent.v.01.misrepresent'), Lemma('misrepresent.v.01.belie')]"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('misrepresent.v.01').lemmas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
